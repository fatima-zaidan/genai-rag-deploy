What is LLM ?

LLM stands for Large Language Model. It’s a type of artificial intelligence (AI) that uses machine learning to understand and generate human language.
These models are trained on vast amounts of text data and can perform various natural language processing tasks like translation, summarization, and content generation
This Article explore what are LLM models,it’s important and Architecture, how they work , how they trained, it’s applications and future works

Why are large language models important?

Large language models are incredibly flexible. One model can perform completely different tasks such as answering questions, summarizing documents, translating languages and completing sentences. LLMs have the potential to disrupt content creation and the way people use search engines and virtual assistants.

While not perfect, LLMs are demonstrating a remarkable ability to make predictions based on a relatively small number of prompts or inputs. LLMs can be used for generative AI (artificial intelligence) to produce content based on input prompts in human language.

LLMs can consider billions of parameters and have many possible uses. Here are some examples

• Open AI’s GPT-3 model has 175 billion parameters. Its cousin, ChatGPT, can identify patterns from data and generate natural and readable output. While we don’t know the size of Claude 2, it can take inputs up to 100K tokens in each prompt, which means it can work over hundreds of pages of technical documentation or even an entire book.
• AI21 Labs’ Jurassic-1 model has 178 billion parameters and a token vocabulary of 250,000-word parts and similar conversational capabilities.
• Cohere’s Command model has similar capabilities and can work in more than 100 different languages

LLM Architecture

LLM (Large Language Model) architecture, primarily based on the Transformer model, leverages self-attention to analyze word relationships in a sentence, enabling efficient and contextually appropriate text processing and generation.

Core Components of LLM Architecture

Tokenization: Text is split into subwords/tokens.

Embeddings: Each token is mapped to a high-dimensional vector.

Positional Encoding: Adds position information to each token embedding.

Architecture Variants:

Transformer-based LLMs typically follow one of three structural patterns:

Encoder-Only (e.g. BERT): These models use only the encoder stack of Transformer layers. All tokens can attend to each other bidirectionally (no causal mask). During pre-training, some tokens are masked out and predicted, so the model learns deep contextual representations. Encoder-only models excel at understanding tasks (classification, extractive QA, sentence embedding, etc.), but they cannot generate arbitrary-length text directly because they lack a decoder. BERT (340M or 110M parameters) is the classic example.

Decoder-Only (e.g. GPT): These stack only decoder layers, with causal (left-to-right) self-attention so each token sees only earlier tokens. In effect, the model is trained to predict the next token in sequence, making it an autoregressive generator. The GPT series (GPT-1 through GPT-4) are decoder-only Transformers. GPT models are powerful at open-ended text generation, conversation, and completion tasks, since they can generate fluent long text continuations. For instance, GPT-3 (175B) and GPT-4 (unknown size) are built this way, with GPT-4 also accepting image inputs.

Encoder–Decoder (Seq2Seq, e.g. T5, BART): These combine both an encoder stack (processing the input sequence) and a decoder stack (generating output). The decoder uses causal self-attention plus cross-attention over the encoder’s final representations. Architecturally this is the original Transformer used for machine translation. Encoder–decoder models are ideal for “sequence-to-sequence” tasks such as translation, summarization, or any setting where a specific output (often shorter) is generated from an input. For example, Google’s T5 treats every task as text-to-text in an encoder-decoder Transformer. Compared to encoder-only or decoder-only, this design explicitly separates “understanding the input” (encoder) from “generating the output” (decoder).

Get Anandhivasudevan’s stories in your inbox
Join Medium for free to get updates from this writer.

Enter your email
Subscribe
In Summary encoder-only models like BERT (bidirectional) are tailored to understanding and classification tasks, decoder-only models like GPT are tailored to generative tasks, and encoder–decoder models (like T5/BART) handle conversion from one sequence to another.

How do large language models work?

A key factor in how LLMs work is the way they represent words. Earlier forms of machine learning used a numerical table to represent each word. But, this form of representation could not recognize relationships between words such as words with similar meanings. This limitation was overcome by using multi-dimensional vectors, commonly referred to as word embeddings, to represent words so that words with similar contextual meanings or other relationships are close to each other in the vector space.

Using word embeddings, transformers can pre-process text as numerical representations through the encoder and understand the context of words and phrases with similar meanings as well as other relationships between words such as parts of speech. It is then possible for LLMs to apply this knowledge of the language through the decoder to produce a unique output.

How are large language models trained?

Transformer-based neural networks are very large. These networks contain multiple nodes and layers. Each node in a layer has connections to all nodes in the subsequent layer, each of which has a weight and a bias. Weights and biases along with embeddings are known as model parameters. Large transformer-based neural networks can have billions and billions of parameters. The size of the model is generally determined by an empirical relationship between the model size, the number of parameters, and the size of the training data.

Training is performed using a large corpus of high-quality data. During training, the model iteratively adjusts parameter values until the model correctly predicts the next token from an the previous sequence of input tokens. It does this through self-learning techniques which teach the model to adjust parameters to maximize the likelihood of the next tokens in the training examples.

Once trained, LLMs can be readily adapted to perform multiple tasks using relatively small sets of supervised data, a process known as fine tuning.

Three common learning models exist:

• Zero-shot learning: LLMs can respond to a broad range of requests without explicit training, often through prompts, although answer accuracy varies.
• Few-shot learning: By providing a few relevant training examples, base model performance significantly improves in that specific area.
• Fine-tuning: This is an extension of few-shot learning in that data scientists train a base model to adjust its parameters with additional data relevant to the specific application.

What are applications of large language models?

There are many practical applications for LLMs.

Copywriting: part from GPT-3 and ChatGPT, Claude, Llama 2, Cohere Command, and Jurassiccan write original copy. AI21 Wordspice suggests changes to original sentences to improve style and voice.

Text classification: Using clustering, LLMs can classify text with similar meanings or sentiments. Uses include measuring customer sentiment, determining the relationship between texts, and document search.

Code generation: LLM are proficient in code generation from natural language prompts. Examples include Amazon CodeWhisperer and Open AI’s codex used in GitHub Copilot, which can code in Python, JavaScript, Ruby and several other programming languages. Other coding applications include creating SQL queries, writing shell commands and website design. Learn more about AI code generation.

Text generation: Similar to code generation, text generation can complete incomplete sentences, write product documentation or, like Alexa Create, write a short children’s story.

What is the future of LLMs?

Conversational AI: LLMs will undoubtedly improve the performance of automated virtual assistants like Alexa, Google Assistant, and Siri. They will be better able to interpret user intent and respond to sophisticated commands.

Increased capabilities : As impressive as they are, the current level of technology is not perfect and LLMs are not infallible. However, newer releases will have improved accuracy and enhanced capabilities as developers learn how to improve their performance while reducing bias and eliminating incorrect answers.

Workplace transformation : LLMs are a disruptive factor that will change the workplace. LLMs will likely reduce monotonous and repetitive tasks in the same way that robots did for repetitive manufacturing tasks. Possibilities include repetitive clerical tasks, customer service chatbots, and simple automated copywriting.