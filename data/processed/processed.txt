Introduction to AI ethics Imagine an AI system that predicts the likelihood of future criminal behavior and is used by judges to determine sentencing lengths. What happens if this system disproportionately targets certain demographic groups? AI ethics is a force for good that helps mitigate unfair biases, removes barriers to accessibility, and augments creativity, among many other benefits
. As organizations increasingly rely on AI for decisions that impact human lives, it’s critical that they consider the complex ethical implications because misusing AI can cause harm to individuals and society—and to businesses’ bottom lines and reputations
. In this article, we’ll explore: Common AI ethics principles, terms, and definitions Creating ethical AI principles for an organization Who’s responsible for AI ethics Implementing AI ethics training, governance, and technical processes Ethical AI use cases and implementations Some leading authorities on the ethics of AI Examples of ethical AI principles The well-being of people is at the center of any discussion about the ethics of AI
. While AI systems can be designed to prioritize morality and ethics, humans are ultimately responsible for ensuring ethical design and use—and to intervene when necessary. There’s no single, universally agreed-upon set of ethical AI principles. Many organizations and government agencies consult with experts in ethics, law, and AI to create their guiding principles
. These principles commonly address: Human wellbeing and dignity: AI systems should always prioritize and ensure the wellbeing, safety, and dignity of individuals, neither replacing humans nor compromising human welfare Human oversight: AI needs human monitoring at every stage of development and use—sometimes called “a human in the loop”—to ensure that ultimate ethical responsibility rests with a human being Addressing bias and discrimination: Design processes should prioritize fairness,
Design processes should prioritize fairness, equality, and representation to mitigate bias and discrimination Transparency and explainability: How AI models make specific decisions and produce specific results should be transparent and explainable in clear language Upholding data privacy and protection: AI systems must meet the most stringent data privacy and protection standards, using robust cybersecurity methods to avoid data breaches and unauthorized access Promoting inclusivity and
and unauthorized access Promoting inclusivity and diversity: AI technologies need to reflect and respect the vast range of human identities and experiences Society and economies: AI should help drive societal advancement and economic prosperity for all people, without fostering inequality or unfair practices Enhancing digital skills and literacy: AI technologies should strive to be accessible and understandable to everyone, regardless of a person’s digital skills The health of businesses: AI
digital skills The health of businesses: AI business technologies should accelerate processes, maximize efficiency, and promote growth AI ethics terms and definitions As an intersection of ethics and high technology, conversations about ethical AI often use vocabulary from both fields
. Understanding this vocabulary is important for being able to discuss the ethics of AI: AI: The ability of a machine to perform cognitive functions we associate with human minds, such as perceiving, reasoning, learning, and problem solving
. There are two main types of AI systems, and some systems are a combination of both: Rule-based AI, also called expert AI, behaves according to a set of fully defined rules created by human experts—as an example, many e-commerce platforms use rule-based AI to provide product recommendations Learning-based AI solves problems and adapts its functionality on its own, based on its initial human-designed configuration and training dataset—generative AI tools are examples of learning-based AI AI
AI tools are examples of learning-based AI AI ethics: A set of values, principles, and techniques that employ widely accepted standards of right and wrong to guide moral conduct in the development, deployment, use, and sale of AI technologies
. AI model: A mathematical framework created by people and trained on data that enables AI systems to perform certain tasks by identifying patterns, making decisions, and predicting outcomes. Common uses include image recognition and language translation, among many others. AI system: A complex structure of algorithms and models designed to mimic human reasoning and perform tasks autonomously. Agency: The capacity of individuals to act independently and to make free choices
. Bias: An inclination or prejudice for or against a person or group, especially in a way considered to be unfair. Biases in training data—such as the under- or over-representation of data pertaining to a certain group—can cause AI to act in biased ways. Explainability: The ability to answer the question, “What did the machine do to reach its output?” Explainability refers to the technological context of the AI system, such as its mechanics, rules and algorithms, and training data
. Fairness: Impartial and just treatment or behavior without unjust favoritism or discrimination. Human-in-the-loop: The ability of human beings to intervene in every decision cycle of an AI system. Interpretability: The ability for people to understand the real-life context and impact of an AI system’s output, such as when AI is used to help make a decision about approving or rejecting a loan application
. Large language model (LLM): A type of machine learning often used in text recognition and generation tasks. Machine learning: A subset of AI that provides systems the ability to automatically learn, improve from experience, and adapt to new data without being explicitly programmed to do so. Normative: A key context of practical ethics concerned with what people and institutions “should” or “ought” to do in particular situations
. Transparency: Related to explainability, transparency is the ability to justify how and why an AI system is developed, implemented, and used, and to make that information visible and understandable to people. How to implement principles for AI ethics For organizations, there’s more to using AI ethically than just adopting ethical principles; these principles must be integrated into all technical and operational AI processes
. While integrating ethics might seem cumbersome for organizations rapidly adopting AI, real-world cases of harm caused by issues in AI model designs and usage show that neglecting proper ethics can be risky and costly. Who’s responsible for AI ethics? The short answer: everyone who’s involved in AI, including businesses, governments, consumers, and citizens
. The different roles of different people in AI ethics Developers and researchers play a crucial role in creating AI systems which prioritize human agency and oversight, address bias and discrimination, and are transparent and explainable. Policymakers and regulators establish laws and regulations to govern the ethical use of AI and protect individuals' rights
. Business and industry leaders ensure their organizations adopt ethical AI principles so that they’re using AI in ways that contribute positively to society. Civil society organizations advocate for the ethical use of AI, play a role in oversight, and provide support for affected communities. Academic institutions contribute through education, research, and the development of ethical guidelines
. End users and affected users, like consumers and citizens, have a stake in ensuring that AI systems are explainable, interpretable, fair, transparent, and beneficial to society. What human stakeholders need to understand infographic The role of business leaders in AI ethics Many businesses establish committees led by their senior leaders to shape their AI governance policies
. For instance, at SAP, we formed an advisory panel and an AI ethics steering committee, consisting of ethics and technology experts, to integrate our ethical AI principles throughout our products and operations
. These principles prioritize: Proportionality and doing no harm Safety and security Fairness and non-discrimination Sustainability Right to privacy and data protection Human oversight and determination Transparency and explainability Responsibility and accountability Awareness and technical literacy Multistakeholder and adaptive governance and collaboration Forming an AI ethics steering committee Establishing a steering committee is vital for managing an organization’s approach to the ethics of
an organization’s approach to the ethics of AI and provides top-level accountability and oversight
. This committee ensures ethical considerations are woven into AI development and deployment. Best practices for forming an AI ethics steering committee Composition and expertise: Include a diverse mix of stakeholders with expertise in AI, law, and ethics. External advisors can offer unbiased perspectives. Defining the purpose and scope: Clearly define the committee’s mission and objectives, focusing on ethical AI design, implementation, and operation
. This should align with the company values, fairness, transparency, and privacy. Defining roles and responsibilities: Outline specific roles for the members, such as developing AI ethics policies, advising on ethics concerns in AI projects, and ensuring compliance with regulations. Setting objectives: Set clear, measurable goals like conducting an annual ethics audit of AI projects and offering quarterly ethical AI training
. Creating procedures: Establish operational procedures, including meeting schedules, documentation standards, and communication protocols to maintain transparency. Ongoing education and adaptation: Keep abreast of new developments in AI technology, ethical standards, and regulations through regular training and conferences. Creating an AI ethics policy Developing an AI ethics policy is essential for guiding AI initiatives within an organization
. The steering committee is critical in this process, using its diverse expertise to ensure the policy adheres to laws, standards, and broader ethical principles. Example approach for creating an AI ethics policy Drafting the initial policy: Begin by drafting a policy that mirrors the organization’s core values, legal requirements, and best practices. This initial draft will serve as the basis for further refinement
. Consultation and input: Engage with internal and external stakeholders, including AI developers, business leaders, and ethicists, to make the policy comprehensive and representative of multiple perspectives. Integration of interdisciplinary insights: Utilize the varied backgrounds of committee members to incorporate insights from technology, ethics, law, and business to address the complex aspects of AI ethics
. Defining high-risk and red-line use cases: To ensure clarity, the committee should outline which AI applications pose significant risks or are considered unethical and, therefore, prohibited
. The SAP Steering Committee, for example, categorizes these as: High-risk: This category includes applications that can be harmful in any way, and includes applications related to law enforcement, migration, and democratic processes—as well as those involving personal data, automated decision-making, or impacting social well-being. These must undergo thorough assessment by the committee before development, deployment, or sale
. Red line: Applications enabling human surveillance, discrimination, deanonymization of data leading to individual or group identification, or those manipulating public opinion or undermining democratic debates are banned. SAP deems these uses highly unethical and prohibits their development, deployment, and sale. Review and revisions: Continuously review and revise the policy based on feedback, ensuring it remains relevant and practical for the real world
. Finalization and approval: Submit the completed policy for final approval by decision-makers, such as the board of directors, backed by a strong recommendation from the committee. Implementation and ongoing oversight: The committee should monitor the policy's implementation and periodically update it to reflect new technological and ethical developments
. Risk Classification & Assessment Process Flowchart Establishing a compliance review process Developing effective compliance review processes is essential to ensure AI deployments adhere to the organization's AI ethics policies and regulations. These processes help build trust with users and regulators and serve to mitigate risks and uphold ethical practices across AI projects
. Typical compliance review processes Develop a standardized review framework: Formulate a comprehensive framework that defines procedures for assessing AI projects against ethical guidelines, legal standards, and operational requirements. Risk classification: Classify AI projects by their ethical and regulatory risks. High-risk projects, such as those handling sensitive personal data or with significant decision-making impacts, require a high degree of scrutiny
. Regular audits and assessments: Perform regular audits to verify ongoing compliance, involving both automated checks and manual reviews by interdisciplinary teams. Stakeholder involvement: Engage a diverse group of stakeholders in the review process, including ethicists, legal experts, data scientists, and end-users, to spot potential risks and ethical dilemmas
. Documentation and transparency: Keep detailed records of all compliance activities, ensuring they are accessible and clear for both internal and external audits Feedback and escalation mechanisms: Implement clear procedures for reporting and addressing ethical concerns and compliance issues Technical implementation of AI ethics practices Integrating ethical considerations into AI development involves adapting current technology practices to ensure systems are built and deployed responsibly
. In addition to establishing ethical AI principles, organizations sometimes also create responsible AI principles, which can be more focused on their specific industry and technical use cases. Key technical requirements for ethical AI systems Bias detection and mitigation: Use diverse data sets and statistical methods to detect and correct biases in AI models. Conduct regular audits to monitor bias
. Conduct regular audits to monitor bias. Transparency and explainability: Develop systems that users can easily understand and verify, employing methods like feature importance scores, decision trees, and model-agnostic explanations to improve transparency. Data privacy and security: Ensure data in AI systems is securely managed and complies with privacy laws. Systems must use encryption, anonymization, and secure protocols to safeguard data integrity
. Robust and reliable design: AI systems must be durable and reliable under various conditions, incorporating extensive testing and validation to handle unexpected scenarios effectively. Continuous monitoring and updating: Maintain ongoing monitoring to assess AI performance and ethical compliance, updating systems as needed based on new data or changes in conditions
. Stakeholder engagement and feedback: Involve stakeholders, such as end-users, ethicists, and domain experts, in the design and development processes to collect feedback and ensure the system aligns with ethical and operational requirements. Training the organization in the ethics of AI Comprehensive training is crucial to ensuring that employees understand AI ethics and can responsibly work with AI technologies
. Training also serves to enhance the integrity and effectiveness of the organizations’ AI tools and solutions. Key components of an effective AI training curriculum Comprehensive curriculum development: Use a training curriculum that addresses AI basics, ethical considerations, compliance issues, and practical applications, tailored to different organizational roles from technical staff to executive management
. Role-specific training modules: Provide training modules customized to the unique needs and responsibilities of various departments. For instance, developers might focus on ethical coding practices, while sales and marketing teams learn about AI's implications in customer interactions. Continuous learning and updates: AI is evolving rapidly, so it’s important to keep training programs up to date with the latest developments and best practices
. Interactive and practical learning experiences: Use case studies, simulations, and workshops to illustrate real-world applications and ethical challenges to support theoretical knowledge with practical experience. Assessment and certification: Conduct assessments to gauge employees’ understanding and proficiency in the ethics of AI and consider offering certification to recognize and motivate continuous improvement
. Feedback mechanisms: Set up feedback channels for employees to contribute to the ongoing refinement of training programs, ensuring they meet the evolving needs of the organization. AI ethics use cases for different roles in the organization Everyone in an organization that works with AI-powered applications, or with AI answer engines, should be cautious for the risk of ai bias and work responsibly
. Examples of AI ethics use cases for different roles or departments in corporate businesses are: Data scientists or machine learning engineers: In these roles, it is recommended to incorporate methods for bias detection and mitigation, ensuring model explainability, and enhancing model. This involves techniques like fairness metrics and counterfactual analysis
. Product managers or business analysts: AI ethic-related responsibilities can vary from ethical risk assessments, prioritizing user-centered design, and developing clear communication strategies to explain AI systems to users and stakeholders. This involves considering potential societal impacts, user needs, and building trust through transparency. Legal & compliance department: Critical use cases are compliance with relevant regulations (e.g
.g., data privacy laws), managing legal and reputational risks associated with AI, and developing strategies to mitigate liabilities arising from algorithmic bias or unintended consequences HR professionals: The HR department should work with AI-powered recruitment tools that are free from bias and comply with anti-discrimination laws. Tasks involve auditing algorithms, implementing human-in-the-loop systems, and providing training on ethical AI recruitment practices
. Authorities on AI ethics AI ethics is complex, shaped by evolving regulations, legal standards, industry practices, and technological advancements. Organizations must stay up to date on policy changes that may impact them—and they should work with relevant stakeholders to determine which policies apply to them. The list below is not exhaustive but provides a sense of the range of policy resources organizations should seek out based on their industry and region
. Examples of AI ethics authorities and resources ACET Artificial Intelligence for Economic Policymaking report: This research study by the African Center for Economic Transformation assesses the economic and ethical considerations of AI for the purpose of informing inclusive and sustainable economic, financial, and industrial policies across Africa
. AlgorithmWatch: A human rights organization that advocates and develops tools for the creation and use of algorithmic systems which protect democracy, the rule of law, freedom, autonomy, justice, and equality. ASEAN Guide on AI Governance and Ethics: A practical guide for member states in the Association of Southeast Asian Nations to design, develop, and deploy AI technologies ethically and productively
. European Commission AI Watch: The European Commission’s Joint Research Centre provides guidance for creating trustworthy AI systems, including country-specific reports and dashboards to help monitor the development, uptake, and impact of AI for Europe NTIA AI Accountability Report: This National Telecommunications and Information Administration report proposes voluntary, regulatory, and other measures to help ensure legal and trustworthy AI systems in the United States
. OECD AI Principles: This forum of countries and stakeholder groups works to shape trustworthy AI. In 2019, it facilitated the OECD AI Principles, the first intergovernmental standard on AI. These principles also served as the basis for the G20 AI Principles. UNESCO Recommendation on the Ethics of Artificial Intelligence: This United Nations agency’s recommendation framework was adopted by 193 member states after a two-year global consultation process with experts and stakeholders
. Conclusion In conclusion, ethical AI development and deployment require a multi-faceted approach. As an organization, is it recommended to establish clear ethical principles, integrate them into AI development processes, and ensure ongoing compliance through robust governance and training programs
. By prioritizing human-centered values like fairness, transparency, and accountability, businesses can harness the power of AI responsibly, driving innovation while mitigating potential risks and ensuring that these technologies benefit society as a whole.
We are introducing GPT‑5, our best AI system yet. GPT‑5 is a significant leap in intelligence over all our previous models, featuring state-of-the-art performance across coding, math, writing, health, visual perception, and more. It is a unified system that knows when to respond quickly and when to think longer to provide expert-level responses
. GPT‑5 is available to all users, with Plus subscribers getting more usage, and Pro subscribers getting access to GPT‑5 pro, a version with extended reasoning for even more comprehensive and accurate answers
. One unified system GPT‑5 is a unified system with a smart, efficient model that answers most questions, a deeper reasoning model (GPT‑5 thinking) for harder problems, and a real‑time router that quickly decides which to use based on conversation type, complexity, tool needs, and your explicit intent (for example, if you say “think hard about this” in the prompt)
. The router is continuously trained on real signals, including when users switch models, preference rates for responses, and measured correctness, improving over time. Once usage limits are reached, a mini version of each model handles remaining queries. In the near future, we plan to integrate these capabilities into a single model
. A smarter, more widely useful model GPT‑5 not only outperforms previous models on benchmarks and answers questions more quickly, but—most importantly—is more useful for real-world queries. We’ve made significant advances in reducing hallucinations, improving instruction following, and minimizing sycophancy, while leveling up GPT‑5’s performance in three of ChatGPT’s most common uses: writing, coding, and health. Coding GPT‑5 is our strongest coding model to date
. It shows particular improvements in complex front‑end generation and debugging larger repositories. It can often create beautiful and responsive websites, apps, and games with an eye for aesthetic sensibility in just one prompt, intuitively and tastefully turning ideas into reality. Early testers also noted its design choices, with a much better understanding of things like spacing, typography, and white space. See here for full details on what GPT‑5 unlocks for developers
. Here are some examples of what GPT‑5 has created with just one prompt: Creative expression and writing GPT‑5 is our most capable writing collaborator yet, able to help you steer and translate rough ideas into compelling, resonant writing with literary depth and rhythm. It more reliably handles writing that involves structural ambiguity, such as sustaining unrhymed iambic pentameter or free verse that flows naturally, combining respect for form with expressive clarity
. These improved writing capabilities mean that ChatGPT is better at helping you with everyday tasks like drafting and editing reports, emails, memos, and more. The writing styles of GPT‑5 and GPT‑4o can be compared in the table below. Health GPT‑5 is our best model yet for health-related questions, empowering users to be informed about and advocate for their health
. The model scores significantly higher than any previous model on HealthBench⁠, an evaluation we published earlier this year based on realistic scenarios and physician-defined criteria. Compared to previous models, it acts more like an active thought partner, proactively flagging potential concerns and asking questions to give more helpful answers
. The model also now provides more precise and reliable responses, adapting to the user’s context, knowledge level, and geography, enabling it to provide safer and more helpful responses in a wide range of scenarios. Importantly, ChatGPT does not replace a medical professional—think of it as a partner to help you understand results, ask the right questions in the time you have with providers, and weigh options as you make decisions. GPT‑5 was trained on Microsoft Azure AI supercomputers
. Building a more robust, reliable, and helpful model More accurate answers to real-world queries GPT‑5 is significantly less likely to hallucinate than our previous models. With web search enabled on anonymized prompts representative of ChatGPT production traffic, GPT‑5’s responses are ~45% less likely to contain a factual error than GPT‑4o, and when thinking, GPT‑5’s responses are ~80% less likely to contain a factual error than OpenAI o3
. We’ve particularly invested in making our models more reliable when reasoning on complex, open-ended questions. Accordingly, we’ve added new evaluations to stress‑test open-ended factuality. We measured GPT‑5’s hallucination rate when thinking on open-ended fact-seeking prompts from two public factuality benchmarks: LongFact⁠(opens in a new window) (concepts and objects) and FActScore⁠(opens in a new window)
. Across all of these benchmarks, “GPT‑5 thinking” shows a sharp drop in hallucinations—about six times fewer than o3—marking a clear leap forward in producing consistently accurate long-form content. Implementation and grading details for our evaluations on these benchmarks can be found in the system card
. Response-level error rate onde-identified ChatGPT traffic More honest responses Alongside improved factuality, GPT‑5 (with thinking) more honestly communicates its actions and capabilities to the user—especially for tasks which are impossible, underspecified, or missing key tools. In order to achieve a high reward during training, reasoning models may learn to lie about successfully completing a task or be overly confident about an uncertain answer
. For example, to test this, we removed all the images from the prompts of the multimodal benchmark CharXiv, and found that OpenAI o3 still gave confident answers about non-existent images 86.7% of the time, compared to just 9% for GPT‑5. When reasoning, GPT‑5 more accurately recognizes when tasks can’t be completed and communicates its limits clearly
. We evaluated deception rates on settings involving impossible coding tasks and missing multimodal assets, and found that GPT‑5 (with thinking) is less deceptive than o3 across the board. On a large set of conversations representative of real production ChatGPT traffic, we’ve reduced rates of deception from 4.8% for o3 to 2.1% of GPT‑5 reasoning responses
.8% for o3 to 2.1% of GPT‑5 reasoning responses. While this represents a meaningful improvement for users, more work remains to be done, and we’re continuing research into improving the factuality and honesty of our models. Further details can be found in the system card. Safer, more helpful responses GPT‑5 advances the frontier on safety. In the past, ChatGPT relied primarily on refusal-based safety training: based on the user’s prompt, the model should either comply or refuse
. While this type of training works well for explicitly malicious prompts, it can struggle to handle situations where the user’s intent is unclear, or information could be used in benign or malicious ways. Refusal training is especially inflexible for dual-use domains such as virology, where a benign request can be safely completed at a high level, but might enable a bad actor if completed in detail
. For GPT‑5, we introduced a new form of safety-training — safe completions — which teaches the model to give the most helpful answer where possible while still staying within safety boundaries. Sometimes, that may mean partially answering a user’s question or only answering at a high level. If the model needs to refuse, GPT‑5 is trained to transparently tell you why it is refusing, as well as provide safe alternatives
. In both controlled experiments and our production models, we find that this approach is more nuanced, enabling better navigation of dual-use questions, stronger robustness to ambiguous intent, and fewer unnecessary overrefusals. Read more about our new approach to safety-training, as well as full details on methodology, metrics, and results, in our safe completion paper⁠. Safety and helpfulness (given safe responses) across prompt intent types
. GPT‑5 (with thinking) demonstrates both higher safety and greater helpfulness across all prompt intent types. Reducing sycophancy and refining style Overall, GPT‑5 is less effusively agreeable, uses fewer unnecessary emojis, and is more subtle and thoughtful in follow‑ups compared to GPT‑4o. It should feel less like “talking to AI” and more like chatting with a helpful friend with PhD‑level intelligence
. Earlier this year, we released an update to GPT‑4o⁠ that unintentionally made the model overly sycophantic, or excessively flattering or agreeable. We quickly rolled back the change⁠ and have since worked to understand and reduce this behavior by: Developing new evaluations to measure sycophancy levels Improving our training so the model is less sycophantic—for instance, adding examples that would normally lead to over-agreement, and then teaching it not to do that
. In targeted sycophancy evaluations using prompts specifically designed to elicit sycophantic responses, GPT‑5 meaningfully reduced sycophantic replies (from 14.5% to less than 6%). At times, reducing sycophancy can come with reductions in user satisfaction, but the improvements we made cut sycophancy by more than half while also delivering other measurable gains, so users continue to have high-quality, constructive conversations—in line with our goal to help people use ChatGPT well⁠
. More ways to customize ChatGPT GPT‑5 is significantly better at instruction following, and we see a corresponding improvement in its ability to follow custom instructions. We’re also launching a research preview of four new preset personalities for all ChatGPT users, made possible by the improvements on steerability
. These personalities, available initially for text chat and coming later to Voice, let you set how ChatGPT interacts—whether concise and professional, thoughtful and supportive, or a bit sarcastic—without writing custom prompts. The four initial options, Cynic, Robot, Listener, and Nerd, are opt-in, adjustable anytime in settings, and designed to match your communication style. All of these new personalities meet or exceed our bar on internal evals for reducing sycophancy
. We look forward to learning and iterating based on early feedback. Comprehensive safeguards for biological risk We decided to treat the “GPT‑5 thinking” model as High capability in the Biological and Chemical domain, and have implemented strong safeguards to sufficiently minimize the associated risks. We rigorously tested the model with our safety evaluations under our Preparedness Framework⁠⁠, completing 5,000 hours of red-teaming with partners like the CAISI and UK AISI
. Similar to our approach for ChatGPT Agent, while we do not have definitive evidence that this model could meaningfully help a novice to create severe biological harm–our defined threshold⁠(opens in a new window) for High capability–we are taking a precautionary approach and are activating the required safeguards now in order to increase readiness for when such capabilities are available
. As a result, “GPT‑5 thinking” has a robust safety stack with a multilayered defense system for biology: comprehensive threat modeling, training the model to not output harmful content through our new safe completions paradigm, always-on classifiers and reasoning monitors, and clear enforcement pipelines. Read more about our robust safety approach for GPT‑5 in our system card
. GPT‑5 pro For the most challenging, complex tasks, we are also releasing GPT‑5 pro, replacing OpenAI o3‑pro, a variant of GPT‑5 that thinks for ever longer, using scaled but efficient parallel test-time compute, to provide the highest quality and most comprehensive answers. GPT‑5 pro achieves the highest performance in the GPT‑5 family on several challenging intelligence benchmarks, including state-of-the-art performance on GPQA, which contains extremely difficult science questions
. In evaluations on over 1000 economically valuable, real-world reasoning prompts, external experts preferred GPT‑5 pro over "GPT‑5 thinking" 67.8% of the time. GPT‑5 pro made 22% fewer major errors and excelled in health, science, mathematics, and coding. Experts rated its responses as relevant, useful, and comprehensive. How to use GPT‑5 GPT‑5 is the new default in ChatGPT, replacing GPT‑4o, OpenAI o3, OpenAI o4-mini, GPT‑4.1, and GPT‑4.5 for signed-in users
.1, and GPT‑4.5 for signed-in users. Just open ChatGPT and type your question; GPT‑5 handles the rest, applying reasoning automatically when the response would benefit from it. Paid users can still select “GPT‑5 Thinking” from the model picker, or type something like ‘think hard about this’ in the prompt to ensure reasoning is used when generating a response
. Availability and access GPT‑5 is starting to roll out today to all Plus, Pro, Team, and Free users, with access for Enterprise and Edu coming next week. Pro, Plus, and Team users can also start coding with GPT‑5 in the Codex CLI⁠(opens in a new window) by signing in with ChatGPT. As with GPT‑4o, the difference between free and paid access to GPT‑5 is usage volume. Pro subscribers get unlimited access to GPT‑5, and access to GPT‑5 Pro
. Plus users can use it comfortably as their default model for everyday questions, with significantly higher usage than free users. Team, Enterprise, and Edu customers can also use GPT‑5 comfortably as their default model for everyday work, with generous limits that make it easy for entire organizations to rely on GPT‑5. For ChatGPT free-tier users, full reasoning capabilities may take a few days to fully roll out
. Once free users reach their GPT‑5 usage limits, they will transition to GPT‑5 mini, a smaller, faster, and highly capable model.
What is LLM ? LLM stands for Large Language Model. It’s a type of artificial intelligence (AI) that uses machine learning to understand and generate human language
. These models are trained on vast amounts of text data and can perform various natural language processing tasks like translation, summarization, and content generation This Article explore what are LLM models,it’s important and Architecture, how they work , how they trained, it’s applications and future works Why are large language models important? Large language models are incredibly flexible
. One model can perform completely different tasks such as answering questions, summarizing documents, translating languages and completing sentences. LLMs have the potential to disrupt content creation and the way people use search engines and virtual assistants. While not perfect, LLMs are demonstrating a remarkable ability to make predictions based on a relatively small number of prompts or inputs
. LLMs can be used for generative AI (artificial intelligence) to produce content based on input prompts in human language. LLMs can consider billions of parameters and have many possible uses. Here are some examples • Open AI’s GPT-3 model has 175 billion parameters. Its cousin, ChatGPT, can identify patterns from data and generate natural and readable output
. While we don’t know the size of Claude 2, it can take inputs up to 100K tokens in each prompt, which means it can work over hundreds of pages of technical documentation or even an entire book. • AI21 Labs’ Jurassic-1 model has 178 billion parameters and a token vocabulary of 250,000-word parts and similar conversational capabilities
. • Cohere’s Command model has similar capabilities and can work in more than 100 different languages LLM Architecture LLM (Large Language Model) architecture, primarily based on the Transformer model, leverages self-attention to analyze word relationships in a sentence, enabling efficient and contextually appropriate text processing and generation. Core Components of LLM Architecture Tokenization: Text is split into subwords/tokens. Embeddings: Each token is mapped to a high-dimensional vector
. Positional Encoding: Adds position information to each token embedding. Architecture Variants: Transformer-based LLMs typically follow one of three structural patterns: Encoder-Only (e.g. BERT): These models use only the encoder stack of Transformer layers. All tokens can attend to each other bidirectionally (no causal mask). During pre-training, some tokens are masked out and predicted, so the model learns deep contextual representations
. Encoder-only models excel at understanding tasks (classification, extractive QA, sentence embedding, etc.), but they cannot generate arbitrary-length text directly because they lack a decoder. BERT (340M or 110M parameters) is the classic example. Decoder-Only (e.g. GPT): These stack only decoder layers, with causal (left-to-right) self-attention so each token sees only earlier tokens. In effect, the model is trained to predict the next token in sequence, making it an autoregressive generator
. The GPT series (GPT-1 through GPT-4) are decoder-only Transformers. GPT models are powerful at open-ended text generation, conversation, and completion tasks, since they can generate fluent long text continuations. For instance, GPT-3 (175B) and GPT-4 (unknown size) are built this way, with GPT-4 also accepting image inputs. Encoder–Decoder (Seq2Seq, e.g. T5, BART): These combine both an encoder stack (processing the input sequence) and a decoder stack (generating output)
. The decoder uses causal self-attention plus cross-attention over the encoder’s final representations. Architecturally this is the original Transformer used for machine translation. Encoder–decoder models are ideal for “sequence-to-sequence” tasks such as translation, summarization, or any setting where a specific output (often shorter) is generated from an input. For example, Google’s T5 treats every task as text-to-text in an encoder-decoder Transformer
. Compared to encoder-only or decoder-only, this design explicitly separates “understanding the input” (encoder) from “generating the output” (decoder). Get Anandhivasudevan’s stories in your inbox Join Medium for free to get updates from this writer
. Enter your email Subscribe In Summary encoder-only models like BERT (bidirectional) are tailored to understanding and classification tasks, decoder-only models like GPT are tailored to generative tasks, and encoder–decoder models (like T5/BART) handle conversion from one sequence to another. How do large language models work? A key factor in how LLMs work is the way they represent words. Earlier forms of machine learning used a numerical table to represent each word
. But, this form of representation could not recognize relationships between words such as words with similar meanings. This limitation was overcome by using multi-dimensional vectors, commonly referred to as word embeddings, to represent words so that words with similar contextual meanings or other relationships are close to each other in the vector space
. Using word embeddings, transformers can pre-process text as numerical representations through the encoder and understand the context of words and phrases with similar meanings as well as other relationships between words such as parts of speech. It is then possible for LLMs to apply this knowledge of the language through the decoder to produce a unique output. How are large language models trained? Transformer-based neural networks are very large
. These networks contain multiple nodes and layers. Each node in a layer has connections to all nodes in the subsequent layer, each of which has a weight and a bias. Weights and biases along with embeddings are known as model parameters. Large transformer-based neural networks can have billions and billions of parameters. The size of the model is generally determined by an empirical relationship between the model size, the number of parameters, and the size of the training data
. Training is performed using a large corpus of high-quality data. During training, the model iteratively adjusts parameter values until the model correctly predicts the next token from an the previous sequence of input tokens. It does this through self-learning techniques which teach the model to adjust parameters to maximize the likelihood of the next tokens in the training examples
. Once trained, LLMs can be readily adapted to perform multiple tasks using relatively small sets of supervised data, a process known as fine tuning. Three common learning models exist: • Zero-shot learning: LLMs can respond to a broad range of requests without explicit training, often through prompts, although answer accuracy varies. • Few-shot learning: By providing a few relevant training examples, base model performance significantly improves in that specific area
. • Fine-tuning: This is an extension of few-shot learning in that data scientists train a base model to adjust its parameters with additional data relevant to the specific application. What are applications of large language models? There are many practical applications for LLMs. Copywriting: part from GPT-3 and ChatGPT, Claude, Llama 2, Cohere Command, and Jurassiccan write original copy. AI21 Wordspice suggests changes to original sentences to improve style and voice
. Text classification: Using clustering, LLMs can classify text with similar meanings or sentiments. Uses include measuring customer sentiment, determining the relationship between texts, and document search. Code generation: LLM are proficient in code generation from natural language prompts. Examples include Amazon CodeWhisperer and Open AI’s codex used in GitHub Copilot, which can code in Python, JavaScript, Ruby and several other programming languages
. Other coding applications include creating SQL queries, writing shell commands and website design. Learn more about AI code generation. Text generation: Similar to code generation, text generation can complete incomplete sentences, write product documentation or, like Alexa Create, write a short children’s story. What is the future of LLMs? Conversational AI: LLMs will undoubtedly improve the performance of automated virtual assistants like Alexa, Google Assistant, and Siri
. They will be better able to interpret user intent and respond to sophisticated commands. Increased capabilities : As impressive as they are, the current level of technology is not perfect and LLMs are not infallible. However, newer releases will have improved accuracy and enhanced capabilities as developers learn how to improve their performance while reducing bias and eliminating incorrect answers. Workplace transformation : LLMs are a disruptive factor that will change the workplace
. LLMs will likely reduce monotonous and repetitive tasks in the same way that robots did for repetitive manufacturing tasks. Possibilities include repetitive clerical tasks, customer service chatbots, and simple automated copywriting.
What is machine learning operations? MLOps, short for machine learning operations, is a set of practices designed to create an assembly line for building and running machine learning models. It helps companies automate tasks and deploy models quickly, ensuring everyone involved (data scientists, engineers, IT) can cooperate smoothly and monitor and improve models for better accuracy and performance. The term MLops is a combination of machine learning (ML) and DevOps
. The term was coined in 2015 in a paper called "Hidden technical debt in machine learning systems," which outlined the challenges inherent in dealing with large volumes of data and how to use DevOps processes to instill better ML practices. Creating an MLOps process incorporates continuous integration and continuous delivery (CI/CD) methodology from DevOps to create an assembly line for each step in creating a machine learning product
. MLOps aims to streamline the time and resources it takes to run data science models. Organizations collect massive amounts of data, which holds valuable insights into their operations and their potential for improvement. Machine learning, a subset of artificial intelligence (AI), empowers businesses to leverage this data with algorithms that uncover hidden patterns that reveal insights
. However, as ML becomes increasingly integrated into everyday operations, managing these models effectively becomes paramount to ensure continuous improvement and deeper insights. Before the advent of MLOps, managing the ML lifecycle was a slow and laborious process, primarily due to the large datasets required in building business applications
. Traditional ML development involves: Significant resources: ML projects require substantial computational power, storage and specialized software, making them expensive to maintain. Hands-on time: Data scientists devote considerable time manually configuring and maintaining models, hindering their ability to focus on innovation. Disparate team involvement: Data scientists, software engineers and IT operations often work in silos, leading to inefficiencies and communication gaps
. By adopting a collaborative approach, MLOps bridges the gap between data science and software development. It leverages automation, CI/CD and machine learning to streamline ML systems' deployment, monitoring and maintenance. This approach fosters close collaboration among data scientists, software engineers and IT staff, ensuring a smooth and efficient ML lifecycle
. 3D design of balls rolling on a track The latest AI News + Insights Discover expertly curated insights and news on AI, cloud and more in the weekly Think Newsletter. Subscribe today How does ML relate to MLOps? Machine learning and MLOps are intertwined concepts but represent different stages and objectives within the overall process. ML focuses on the technical nuances of crafting and refining models
. The overarching aim is to develop accurate models capable of undertaking various tasks such as classification, prediction or providing recommendations, ensuring that the end product efficiently serves its intended purpose. "MLOps emphasizes the comprehensive management of the machine learning model lifecycle, covering everything from deploying models into production environments to monitoring their performance
. When necessary, models are updated to ensure that they continue to function effectively. The goal is to streamline the deployment process, guarantee models operate at their peak efficiency and foster an environment of continuous improvement. By focusing on these areas, MLOps ensures that machine learning models meet the immediate needs of their applications and adapt over time to maintain relevance and effectiveness in changing conditions
. While ML focuses on the technical creation of models, MLOps focuses on the practical implementation and ongoing management of those models in a real-world setting. ML models operate silently within the foundation of various applications, from recommendation systems that suggest products to chatbots automating customer service interactions. ML also enhances search engine results, personalizes content and improves automation efficiency in areas like spam and fraud detection
. Virtual assistants and smart devices leverage ML’s ability to understand spoken language and perform tasks based on voice requests. ML and MLOps are complementary pieces that work together to create a successful machine-learning pipeline. AI Academy AI Academy 2024 Teaser Trailer Become an AI expert Gain the knowledge to prioritize AI investments that drive business growth. Get started with our free AI Academy today and lead the future of AI in your organization
. Watch the series The benefits of MLOps MLOps streamlines model creation to improve efficiency, boost accuracy, accelerate time to market and ensure scalability and governance. Increased efficiency MLOps automates manual tasks, freeing up valuable time and resources for data scientists and engineers to focus on higher-level activities like model development and innovation
. For example, without MLOps, a personalized product recommendation algorithm requires data scientists to manually prepare and deploy data into production. At the same time, operations teams must monitor the model's performance and manually intervene if issues arise. This process is time-consuming, prone to human error and difficult to scale
. Improved model accuracy and performance MLOps facilitates continuous monitoring and improvement of models, allowing for faster identification and rectification of issues, leading to more accurate and reliable models. Without MLOps, fraud analysts must manually analyze data to build rules for detecting fraudulent transactions. These static models are helpful but are susceptible to data drift, causing the model's performance to degrade
. Faster time to market By streamlining the ML lifecycle, MLOps enables businesses to deploy models faster, gaining a competitive edge in the market. Traditionally, developing a new machine-learning model can take weeks or months to ensure that each step of the process is done correctly. The data must be prepared and the ML model must be built, trained, tested and approved for production. In an industry like healthcare, the risk of approving a faulty model is too significant to do otherwise
. Scalability and governance MLOps establishes a defined and scalable development process, ensuring consistency, reproducibility and governance throughout the ML lifecycle. Manual deployment and monitoring are slow and require significant human effort, hindering scalability. Without proper centralized monitoring, individual models might experience performance issues that go unnoticed, impacting overall accuracy
. What's the relationship to DevOps? MLOps and DevOps focus on different aspects of the development process. DevOps focuses on streamlining the development, testing and deployment of traditional software applications. It emphasizes collaboration between development and operations teams to automate processes and improve software delivery speed and quality. MLOps builds upon DevOps principles and applies them to the machine learning lifecycle
. It goes beyond deploying code, encompassing data management, model training, monitoring and continuous improvement. While MLOps leverages many of the same principles as DevOps, it introduces supplementary steps and considerations unique to the complexities of building and maintaining machine learning systems
. Core principles of MLOps Adhering to the following principles allows organizations to create a robust and efficient MLOps environment that fully uses the potential inherent within machine learning. 1. Collaboration: MLOps emphasizes breaking down silos between data scientists, software engineers and IT operations. This fosters communication and ensures everyone involved understands the entire process and contributes effectively. 2
. 2. Continuous improvement: MLOps promotes an iterative approach where models are constantly monitored, evaluated and refined. This process ensures that models stay relevant and accurate and address evolving business needs. 3. Automation: Automating repetitive tasks like data preparation, model training and deployment frees up valuable time for data scientists and engineers to focus on higher-level activities like model development and innovation. 4
. 4. Reproducibility: MLOps practices ensure that experiments and deployments are reproducible, allowing for easier debugging, sharing and comparison of results. This approach promotes transparency and facilitates collaboration. 5. Versioning: Effective versioning of data, models and code allows for tracking changes, reverting to previous versions if necessary and ensuring consistency across different stages of the ML lifecycle. 6
. 6. Monitoring and observability: MLOps continuously monitors models' performance, data quality and infrastructure health. This capability enables proactive identification and resolution of issues before they impact production systems. 7. Governance and security: MLOps practices consider compliance with regulations and ethical guidelines while ensuring secure access, data privacy and model safety throughout the ML lifecycle. 8
. 8. Scalability and security: Scalable and secure designs can adapt to growing volumes of data, increased model complexity and the expanding demands of ML projects, ensuring that systems remain robust and efficient as they evolve. What are the key elements of an effective MLOps strategy? MLOps requires skills, tools and practices to effectively manage the machine learning lifecycle. MLOps teams need a diverse skillset encompassing both technical and soft skills
. They must understand the entire data science pipeline, from data preparation and model training to evaluation. Familiarity with software engineering practices like version control, CI/CD pipelines and containerization are also crucial. In addition, knowledge of DevOps principles, infrastructure management and automation tools is essential for the efficient deployment and operation of ML models. Beyond technical expertise, soft skills play a vital role in successful MLOps
. Collaborating effectively with diverse teams (data scientists, machine learning engineers and IT professionals) is critical for smooth collaboration and knowledge sharing. Strong communication skills are necessary to translate technical concepts into clear and concise language for various technical and nontechnical stakeholders. MLOps leverages various tools to simplify the machine learning lifecycle
. Machine learning frameworks like Kubernetes, TensorFlow and PyTorch for model development and training. Version control systems like Git for code and model version tracking. CI/CD tools such as Jenkins or GitLab CI/CD for automating model building, testing and deployment. MLOps platforms like Kubeflow and MLflow manage model lifecycles, deployment and monitoring. Cloud computing platforms like AWS, Azure and IBM Cloud provide scalable infrastructure for running and managing ML workloads
. Effective MLOps practices involve establishing well-defined procedures to ensure efficient and reliable machine learning development. A fundamental aspect of this process is setting up a documented and repeatable sequence of steps for all phases of the ML lifecycle, which promotes clarity and consistency across different teams involved in the project. Furthermore, the versioning and managing of data, models and code are crucial
. By tracking changes and maintaining various versions, teams can easily roll back to previous states, reproduce experiments accurately, stay aware of changes over time and ensure traceability throughout the development cycle. Continuous monitoring of model performance for accuracy drift, bias and other potential issues plays a critical role in maintaining the effectiveness of models and preventing unexpected outcomes
. Monitoring the performance and health of ML models ensures that they continue to meet the intended objectives after deployment. By proactively identifying and addressing these concerns, organizations can maintain optimal model performance, mitigate risks and adapt to changing conditions or feedback. CI/CD pipelines further streamline the development process, playing a significant role in automating the build, test and deployment phases of ML models
. Implementing CI/CD pipelines enhances consistency and efficiency across machine learning projects. It also accelerates delivery cycles, allowing teams to bring innovations to market more quickly while ensuring greater confidence in the reliability of their ML solutions. Automating the build, test and deployment phases of ML models reduces the chances of human error, enhancing the overall reliability of the ML systems. Collaboration is the lifeblood of successful MLOps
. Open communication and teamwork between data scientists, engineers and operations teams are crucial. This collaborative approach breaks down silos, promotes knowledge sharing and ensures a smooth and successful machine-learning lifecycle. By integrating diverse perspectives throughout the development process, MLOps teams can build robust and effective ML solutions that form the foundation of a strong MLOps strategy
. Key components of the MLOps pipeline The MLOps pipeline comprises various components that streamline the machine learning lifecycle, from development to deployment and monitoring. Data management Data management is a critical aspect of the data science lifecycle, encompassing several vital activities. Data acquisition is the first step; raw data is collected from various sources such as databases, sensors and APIs
. This stage is crucial for gathering the information that is the foundation for further analysis and model training. Following the acquisition, data preprocessing is conducted to ensure that the data is in a suitable format for analysis. In this step, the data is cleaned to remove any inaccuracies or inconsistencies and transformed to fit the analysis or model training needs. Handling missing values, normalization, and feature engineering are typical activities in this phase
. These steps aim to enhance the quality and usefulness of the data for predictive modeling. Data versioning plays a pivotal role in maintaining the integrity and reproducibility of data analysis. It involves tracking and managing different versions of the data, allowing for traceability of results and the ability to revert to previous states if necessary. Versioning ensures that others can replicate and verify analyses, promoting transparency and reliability in data science projects
. The concept of a feature store is then introduced as a centralized repository for storing and managing features used in model training. Feature stores promote consistency and reusability of features across different models and projects. By having a dedicated system for feature management, teams can ensure they use the most relevant and up-to-date features
. Model development Model development is a core phase in the data science process, focusing on constructing and refining machine learning models. This phase starts with model training, where the prepared data is used to train machine learning models that use selected algorithms and frameworks. The objective is to teach the model to make accurate predictions or decisions based on the data it has been trained on
. An essential aspect of model development is maintaining and tracking experiments, which involves keeping detailed records of different model iterations, the hyperparameter configurations used and the outcomes of various experiments. Such meticulous documentation is critical for comparing different models and configurations, facilitating the identification of the most effective approaches
. This process helps optimize model performance and ensures that the development process is transparent and reproducible. Following the training phase, model evaluation is conducted to assess the performance of the models on unseen data. Evaluation is critical to ensure that the models perform well in real-world scenarios. Metrics such as accuracy, precision, recall and fairness measures gauge how well the model meets the project objectives
. These metrics provide a quantitative basis for comparing different models and selecting the best one for deployment. Through careful evaluation, data scientists can identify and address potential issues, such as bias or overfitting, ensuring that the final model is effective and fair. Model deployment Bringing a machine learning model to use involves model deployment, a process that moves the model from a development setting to a production environment where it can provide real value
. This step begins with model packaging and deployment, where trained models are prepared for use and deployed to production environments. Production environments can vary, including cloud platforms and on premises servers, depending on the specific needs and constraints of the project. The aim is to ensure that the model is accessible and can operate effectively in a live setting. Once deployed, the focus shifts to model serving, which entails the delivery of output through APIs
. This step must be reliably and efficiently executed to ensure that end users can depend on the model for timely and accurate results. Often, this process requires a well-designed system capable of handling requests at scale and providing low-latency responses to users. Infrastructure management is another critical component of model deployment. Management involves overseeing the underlying hardware and software frameworks that enable the models to run smoothly in production
. Key technologies in this domain include containerization and orchestration tools, which help to manage and scale the models as needed. These tools ensure that the deployed models are resilient and scalable, capable of meeting the demands of production workloads. Through careful deployment and infrastructure management, organizations can maximize the utility and impact of their machine-learning models in real-world applications
. Monitoring and optimization In the lifecycle of a deployed machine learning model, continuous vigilance ensures effectiveness and fairness over time. Model monitoring forms the cornerstone of this phase, involving the ongoing scrutiny of the model's performance in the production environment. This step helps identify emerging issues, such as accuracy drift, bias and concerns around fairness, which could compromise the model's utility or ethical standing
. Monitoring is about overseeing the model's current performance and anticipating potential problems before they escalate. Setting up robust alerting and notification systems is essential to complement the monitoring efforts. These systems serve as an early warning mechanism, flagging any signs of performance degradation or emerging issues with the deployed models
. By receiving timely alerts, data scientists and engineers can quickly investigate and address these concerns, minimizing their impact on the model's performance and the end-users' experience. Insights gained from continuous monitoring and the alerting system feed into the model retraining and improvement process, which involves updating the models with new data or integrating improved algorithms to refine their performance. Retraining models is not a one-time task but a recurring need
. New data can reflect changes in the underlying patterns or relationships data scientists trained the model to recognize. By iteratively improving the models based on the latest data and technological advances, organizations can ensure that their machine-learning solutions remain accurate, fair and relevant, sustaining their value over time
. This cycle of monitoring, alerting and improvement is crucial for maintaining the integrity and efficacy of machine learning models in dynamic real-world environments. Collaboration and governance Creating a streamlined and efficient workflow needs the adoption of several practices and tools, among which version control stands as a cornerstone. Using systems like Git, teams can meticulously track and manage changes in code, data and models
. Fostering a collaborative environment makes it easier for team members to work together on projects and ensures that any modifications can be documented and reversed when needed. The ability to roll back to previous versions is invaluable, especially when new changes introduce errors or reduce the effectiveness of the models. "Complementing the technical rigor of version control, integrating collaboration tools enhances communication and knowledge sharing
. These platforms help diverse stakeholders in the MLOps pipeline, including data science teams, engineers, and other professionals, work more effectively together. By streamlining communication, these tools help align project goals, share insights and resolve issues more efficiently, accelerating the development and deployment processes. At a higher level of operation, the principle of ML governance takes precedence
. This framework involves creating and enforcing policies and guidelines that govern machine learning models' responsible development, deployment and use. Such governance frameworks are critical for ensuring that the models are developed and used ethically, with due consideration given to fairness, privacy and regulatory compliance
. Establishing a robust ML governance strategy is essential for mitigating risks, safeguarding against misuse of technology and ensuring that machine learning initiatives align with broader ethical and legal standards. These practices—version control, collaboration tools and ML governance—collectively form the backbone of a mature and responsible MLOps ecosystem, enabling teams to deliver impactful and sustainable machine learning solutions
. This entire pipeline process is iterative, with insights from monitoring and optimization feeding back into model development and leading to continuous improvement. Collaboration and governance are crucial throughout the lifecycle to ensure smooth execution and responsible use of ML models. Successful implementation and continual support of MLOps requires adherence to a few core best practices
. The priority is establishing a transparent ML development process covering every stage, which includes data selection, model training, deployment, monitoring and incorporating feedback loops for improvement. When team members have insight into these methodologies, the result is smoother transitions between project phases, enhancing the development process's overall efficiency. A pivotal aspect of MLOps is the maintenance and management of data, models and code
. By maintaining distinct versions of these components, teams can effectively keep aware of changes over time, which is essential for troubleshooting issues, ensuring reproducibility of results and facilitating easier rollbacks when necessary. This approach aids in maintaining the integrity of the development process and enables auditability in ML projects. Monitoring the performance and health of ML models is critical to ensure that they continue to meet the intended objectives after deployment
. This process involves regularly assessing for model drift, bias and other potential issues that could compromise their effectiveness. By proactively identifying and addressing these concerns, organizations can maintain optimal model performance, mitigate risks and adapt to changing conditions or feedback. CI/CD pipelines play a significant role in automating and streamlining the build, test and deployment phases of ML models
. Implementing CI/CD pipelines enhances consistency and efficiency across machine learning projects. In addition, it accelerates delivery cycles, enabling teams to bring innovations to market more rapidly and with greater confidence in the reliability of their ML solutions. How generative AI affects MLOps While generative AI (gen AI) has the potential to impact MLOps, it's an emerging field and its concrete effects are still being explored and developed
. Gen AI could enhance the MLOps workflow by automating labor-intensive tasks such as data cleaning and preparation, potentially boosting efficiency and allowing data scientists and engineers to concentrate on more strategic activities. In addition, ongoing research into gen AI might enable the automatic generation and evaluation of machine learning models, offering a pathway to faster development and refinement. However, model transparency and bias issues are yet to be fully addressed
. Integrating gen AI into MLOps is not without its challenges as well. Ensuring models are interpretable and trustworthy is a primary concern, as comprehending how models arrive at their decisions and being able to mitigate biases is vital for responsible AI development. While gen AI presents exciting opportunities for MLOps, it also brings critical issues that need thorough exploration and thoughtful solutions to the forefront
. How are LLMs related to MLOps? Large language models (LLMs) are an advanced machine learning model requiring specialized training and deployment processes, making MLOps methodologies crucial for their lifecycle management. MLOps streamlines LLM development by automating data preparation and model training tasks, ensuring efficient versioning and management for better reproducibility
. MLOps processes enhance LLMs' development, deployment and maintenance processes, addressing challenges like bias and ensuring fairness in model outcomes. Furthermore, LLMs offer potential benefits to MLOps practices, including the automation of documentation, assistance in code reviews and improvements in data preprocessing. These contributions could significantly enhance the efficiency and effectiveness of MLOps workflows. Levels of MLOps There are three levels of MLOps implementation
. Each level is a progression toward greater automation maturity within an organization. Level 0: No MLOps Here's where most organizations start. Models are deployed manually and managed individually, often by data scientists. This approach is inefficient, prone to errors and difficult to scale as projects grow. Imagine building and deploying models like putting together raw furniture one screw at a time-slow, tedious and prone to mistakes
. Level 1: ML pipeline automation The introduction of automation. Scripts or basic CI/CD pipelines handle essential tasks like data preprocessing, model training and deployment. This level brings efficiency and consistency, similar to having a predrilled furniture kit-faster and less error-prone, but still lacking features. Level 2: CI/CD pipeline integration The ML pipeline has been seamlessly integrated with existing CI/CD pipelines
. This level enables continuous model integration, delivery and deployment, making the process smoother and faster. Think of it as having a furniture assembly kit with clear instructions—efficient and quick iterations are now possible. Level 3: Advanced MLOps This level takes things further, incorporating features like continuous monitoring, model retraining and automated rollback capabilities. Collaboration, version control and governance also become vital aspects
. Imagine having a smart furniture system that automatically monitors wear and tear, repairs itself and even updates its fully optimized and robust software, just like a mature MLOps environment. Reaching the "right" level Achieving the highest MLOps level isn't necessary or practical. The optimal level for your organization depends on its specific needs and resources
. However, understanding these levels helps you assess your current state and identify areas for improvement on your MLOps journey—your path toward building an efficient, reliable and scalable machine learning environment. Ultimately, MLOps represents a shift in how organizations develop, deploy and manage machine learning models, offering a comprehensive framework to streamline the entire machine learning lifecycle
. By fostering a collaborative environment that bridges the gap between data scientists, ML engineers and IT professionals, MLOps facilitates the efficient production of ML-powered solutions. It ensures that data is optimized for success at every step, from data collection to real-world application. With its emphasis on continuous improvement, MLOps allows for the agile adaptation of models to new data and evolving requirements, ensuring their ongoing accuracy and relevance
. By applying MLOps practices across various industries, businesses can unlock the full potential of machine learning, from enhancing e-commerce recommendations to improving fraud detection and beyond. The success of MLOps hinges on a well-defined strategy, the right technological tools and a culture that values collaboration and communication.
Multi-Agent System: An Overview In the context of language models and AI, a multi-agent system involves multiple independent actors, each powered by language models, collaborating in a specific way. These agents have their own persona/role, and a context that is define by the prompts on a specific language model. Each agent has access to various tools, to help execute the task given to the agent. Multiple agents bring different perspectives and helps make better decisions
. Multi-agent systems differ from single-agent systems primarily in the distribution of decision-making and interaction within a system. In a single-agent system, a centralized agent makes all decisions, while other agents act as remote slaves. This single agent, normally decides, based on the context. This might miss out the other perspectives/possibilities
. On the other hand, multi-agent systems involve multiple interacting intelligent agents, each capable of making decisions and influencing the environment. The idea behind multi-agent architecture is to create agents, with different contexts to bring in different perspective, by the role they play. Though they might be using the same LLM, but due to the role, goal and the context that is defined for that agent, they behave different. Just like a member in the team
. Just like a member in the team. Imagine you have an agent, that generates application code, another agent that reviews the code and they both get into a chat with each other to improve the code. Over a defined set of iterations, these two agents will come up with the best results. This approach has a huge potential, to not only generate a more satisfactory output, but also reduce the effects of Hallucinations, Bias etc
. Defining the right context, prompt, with right model, in a multi-agent architecture is very critical. Strong prompt engineering skills help in designing an impactful multi-agent application. Benefits of Multi-Agent Designs: Separation of concerns: Each agent can have its own instructions and few-shot examples, powered by separate fine-tuned language models, and supported by various tools. Dividing tasks among agents leads to better results
. Each agent can focus on a specific task rather than selecting from a multitude of tools. Modularity: Multi-agent designs allow breaking down complex problems into manageable units of work, targeted by specialized agents and language models. Multi-agent designs allow you to evaluate and improve each agent independently without disrupting the entire application. Grouping tools and responsibilities can lead to better outcomes. Agents are more likely to succeed when focused on specific tasks
. Diversity: Bring in strong diversity in the agent-teams to bring in different perspectives and refine the output and avoid Hallucinations & Bias. (Like a typical human team). Reusability: Once the agents are built, there is an opportunity to reuse these agents for different use cases, and think of an ecosystem of agents, that can come together to solve the problem, with a proper choreography/orchestration framework (such as AutoGen, Crew
.ai etc) Typical multi-Agent architecture consists of the following components. Agents: Intelligent Agents have a very clear role, persona and context, and runs on an LLM. Connections: How are these agents connected? Orchestration: Orchestration defines how these agents work together (Sequential, Hierarchical, Bi-directional chat etc) Human: We will require Human in the middle in most use cases, to help take decisions and evaluate the results
. Tools: Tools that these agents use to run specific tasks such as search the web for more information, or generate/read the document, or upload the generated code to GitHub etc. LLM: OfCourse this is all backed by specific language models, that the agent uses for inference. In summary, multi-agent architectures empower collaboration among independent language model-powered agents, leading to more effective and modular solutions
. Get A B Vijay Kumar’s stories in your inbox Join Medium for free to get updates from this writer. Enter your email Subscribe There are various frameworks that allow us to build these multi-agent applications. The following are some of these frameworks. However, this is a evolving field, and lot of these frameworks are changing by the day, and new ones are being introduced. But all of them provide easier ways to build and manage multiple agents
. In future blogs, we will be building some applications on these frameworks. OpenAI Assistant: OpenAI Assistant is one of the first frameworks to support a multi-agent architecture. This framework enables the creation of persistent, multimodal multi-agent systems that can interact with users over extended periods. Agents can access files and tools, including a Code Interpreter, and communicate with other agents to perform tasks
. This is ideal for applications that require long-term collaboration/interaction. Autogen: Autogen is one of the popular emerging frameworks by Microsoft. This is an open-source framework, which also comes up with a very intuitive UI based development tool called Autogen Studio. for building robust multi-agent applications. It allows the creation of LLM agents that use Large Language Models for reasoning and action, which can be augmented with information from custom sources
. It provides a very well-defined orchestrator-based approach towards multi-agent architecture. Dragonscale’s Multi-Agent Systems: Focuses on integrating various generative AI models and tools to create intelligent systems capable of managing tasks ranging from simple to highly complex, especially in dynamic business environments. This is suitable for dynamic business environments requiring adaptability. This provides a framework to manage complex tasks
. CrewAI: CrewAI is one of the emerging frameworks, which is gaining popularity, and is being compared with Autogen. CrewAI provides a very good framework for orchestrating role-playing, autonomous AI agents. CrewAI fosters collaborative intelligence, empowering agents to work together seamlessly to tackle complex tasks. It is designed to enable AI agents to assume roles, share goals, and operate in a cohesive unit
. This is one of my favourite frameworks, I am watching this framework closely, and I will blog about the applications, I have built on CrewAI in my next blog. LangGraph: LangGraph is another very powerful and promising multi-agent framework for building stateful, multi-actor applications with LLMs, built on top of LangChain
. It extends the LangChain Expression Language with the ability to coordinate multiple chains (or actors) across multiple steps of computation in a cyclic manner, inspired by Pregel and Apache Beam. LangGraph has the power of a strong community and LangChain ecosystem
. These frameworks are part of the ongoing evolution of AI, where the focus is on creating intelligent systems that can learn from and adapt to their environments, leading to more personalized and efficient solutions across industries.
What is Retrieval-Augmented Generation? Retrieval-Augmented Generation (RAG) is the process of optimizing the output of a large language model, so it references an authoritative knowledge base outside of its training data sources before generating a response. Large Language Models (LLMs) are trained on vast volumes of data and use billions of parameters to generate original output for tasks like answering questions, translating languages, and completing sentences
. RAG extends the already powerful capabilities of LLMs to specific domains or an organization's internal knowledge base, all without the need to retrain the model. It is a cost-effective approach to improving LLM output so it remains relevant, accurate, and useful in various contexts. Why is Retrieval-Augmented Generation important? LLMs are a key artificial intelligence (AI) technology powering intelligent chatbots and other natural language processing (NLP) applications
. The goal is to create bots that can answer user questions in various contexts by cross-referencing authoritative knowledge sources. Unfortunately, the nature of LLM technology introduces unpredictability in LLM responses. Additionally, LLM training data is static and introduces a cut-off date on the knowledge it has. Known challenges of LLMs include: Presenting false information when it does not have the answer
. Presenting out-of-date or generic information when the user expects a specific, current response. Creating a response from non-authoritative sources. Creating inaccurate responses due to terminology confusion, wherein different training sources use the same terminology to talk about different things. You can think of the Large Language Model as an over-enthusiastic new employee who refuses to stay informed with current events but will always answer every question with absolute confidence
. Unfortunately, such an attitude can negatively impact user trust and is not something you want your chatbots to emulate! RAG is one approach to solving some of these challenges. It redirects the LLM to retrieve relevant information from authoritative, pre-determined knowledge sources. Organizations have greater control over the generated text output, and users gain insights into how the LLM generates the response
. What are the benefits of Retrieval-Augmented Generation? RAG technology brings several benefits to an organization's generative AI efforts. Cost-effective implementation Chatbot development typically begins using a foundation model. Foundation models (FMs) are API-accessible LLMs trained on a broad spectrum of generalized and unlabeled data. The computational and financial costs of retraining FMs for organization or domain-specific information are high
. RAG is a more cost-effective approach to introducing new data to the LLM. It makes generative artificial intelligence (generative AI) technology more broadly accessible and usable. Current information Even if the original training data sources for an LLM are suitable for your needs, it is challenging to maintain relevancy. RAG allows developers to provide the latest research, statistics, or news to the generative models
. They can use RAG to connect the LLM directly to live social media feeds, news sites, or other frequently-updated information sources. The LLM can then provide the latest information to the users. Enhanced user trust RAG allows the LLM to present accurate information with source attribution. The output can include citations or references to sources. Users can also look up source documents themselves if they require further clarification or more detail
. This can increase trust and confidence in your generative AI solution. More developer control With RAG, developers can test and improve their chat applications more efficiently. They can control and change the LLM's information sources to adapt to changing requirements or cross-functional usage. Developers can also restrict sensitive information retrieval to different authorization levels and ensure the LLM generates appropriate responses
. In addition, they can also troubleshoot and make fixes if the LLM references incorrect information sources for specific questions. Organizations can implement generative AI technology more confidently for a broader range of applications. How does Retrieval-Augmented Generation work? Without RAG, the LLM takes the user input and creates a response based on information it was trained on—or what it already knows
. With RAG, an information retrieval component is introduced that utilizes the user input to first pull information from a new data source. The user query and the relevant information are both given to the LLM. The LLM uses the new knowledge and its training data to create better responses. The following sections provide an overview of the process. Create external data The new data outside of the LLM's original training data set is called external data
. It can come from multiple data sources, such as a APIs, databases, or document repositories. The data may exist in various formats like files, database records, or long-form text. Another AI technique, called embedding language models, converts data into numerical representations and stores it in a vector database. This process creates a knowledge library that the generative AI models can understand. Retrieve relevant information The next step is to perform a relevancy search
. The user query is converted to a vector representation and matched with the vector databases. For example, consider a smart chatbot that can answer human resource questions for an organization. If an employee searches, "How much annual leave do I have?" the system will retrieve annual leave policy documents alongside the individual employee's past leave record. These specific documents will be returned because they are highly-relevant to what the employee has input
. The relevancy was calculated and established using mathematical vector calculations and representations. Augment the LLM prompt Next, the RAG model augments the user input (or prompts) by adding the relevant retrieved data in context. This step uses prompt engineering techniques to communicate effectively with the LLM. The augmented prompt allows the large language models to generate an accurate answer to user queries
. Update external data The next question may be—what if the external data becomes stale? To maintain current information for retrieval, asynchronously update the documents and update embedding representation of the documents. You can do this through automated real-time processes or periodic batch processing. This is a common challenge in data analytics—different data-science approaches to change management can be used. The following diagram shows the conceptual flow of using RAG with LLMs
. What is the difference between Retrieval-Augmented Generation and semantic search? Semantic search enhances RAG results for organizations wanting to add vast external knowledge sources to their LLM applications. Modern enterprises store vast amounts of information like manuals, FAQs, research reports, customer service guides, and human resource document repositories across various systems. Context retrieval is challenging at scale and consequently lowers generative output quality
. Semantic search technologies can scan large databases of disparate information and retrieve data more accurately. For example, they can answer questions such as, "How much was spent on machinery repairs last year?” by mapping the question to the relevant documents and returning specific text instead of search results. Developers can then use that answer to provide more context to the LLM. Conventional or keyword search solutions in RAG produce limited results for knowledge-intensive tasks
. Developers must also deal with word embeddings, document chunking, and other complexities as they manually prepare their data. In contrast, semantic search technologies do all the work of knowledge base preparation so developers don't have to. They also generate semantically relevant passages and token words ordered by relevance to maximize the quality of the RAG payload.
Fine-Tuning the Model: What, Why, and How As technology continues to advance, machine learning models have become increasingly powerful in solving a wide range of tasks. Fine-tuning a model is one such technique that allows us to adapt pre-trained neural network models for specific tasks or datasets. In this blog post, we will delve into what fine-tuning is, why it is used, and how it can be done effectively. What is Fine-Tuning? Fine-tuning in deep learning is a form of transfer learning
. It involves taking a pre-trained model, which has been trained on a large dataset for a general task such as image recognition or natural language understanding, and making minor adjustments to its internal parameters. The goal is to optimize the model’s performance on a new, related task without starting the training process from scratch. Typically, the overall architecture of the pre-trained model remains mostly intact during the fine-tuning process
. The idea is to leverage the valuable features and representations learned by the model from the vast dataset it was initially trained on and adapt them to tackle a more specific task. Why Use Fine-Tuning? Fine-tuning offers several distinct advantages that have made it a popular technique in the field of machine learning: Efficiency Training a deep learning model from scratch can be extremely time-consuming and computationally expensive
. Fine-tuning, on the other hand, allows us to build upon a pre-trained model, significantly reducing the time and resources required to achieve good results. By starting with a model that has already learned many relevant features, we can skip the initial stages of training and focus on adapting the model to the specific task at hand. Improved Performance Pre-trained models have been trained on vast amounts of data for general tasks
. This means that they have already learned valuable features and patterns that can be beneficial for related tasks. By fine-tuning a pre-trained model, we can leverage this wealth of knowledge and representations, leading to improved performance on our specific task. Data Efficiency In many real-world scenarios, obtaining labeled data for a specific task can be challenging and time-consuming. Fine-tuning offers a solution by allowing us to effectively train models even with limited labeled data
. By starting with a pre-trained model and adapting it to our specific task, we can make the most of the available labeled data and achieve good results with less effort. How to Fine-Tune a Model? Now that we understand what fine-tuning is and why it is advantageous, let’s discuss a step-by-step approach to effectively fine-tuning a model: 1. Select a Pre-trained Model The first step in fine-tuning a model is to choose a pre-trained model that matches the nature of your task
. For example, if you are working on an image classification task, you can start with a pre-trained image classification model. It’s essential to select a model with similar or related features to the task you want to tackle. 2. Adjust the Architecture After selecting the pre-trained model, you need to make modifications to the model’s architecture to fit the requirements of your specific task. This typically involves modifying the top layers of the model
. For example, you may need to change the number of output neurons in the final layer to match the number of classes in your classification task. 3. Freeze or Unfreeze Layers Depending on the complexity of your task and the size of your dataset, you can choose to freeze some layers in the pre-trained model. Freezing a layer means preventing it from updating its weights during the fine-tuning process
. This can be beneficial if the lower layers of the pre-trained model have already learned general features that are useful for your task. On the other hand, unfreezing allows the corresponding layers to adapt to the new data during fine-tuning. 4. Training Once you have adjusted the architecture and decided which layers to freeze or unfreeze, it’s time to train the modified model on your task-specific dataset
. During training, it’s advisable to use a smaller learning rate than what was used in the initial pre-training phase. This helps prevent drastic changes to the already learned representations while allowing the model to adapt to the new data. 5. Fine-Tuning Strategies Every task and dataset is unique, and it may require further experimentation with hyperparameters, loss functions, and other training strategies
. Fine-tuning is not a one-size-fits-all approach, and you may need to iterate and fine-tune your fine-tuning strategy to achieve optimal results. In conclusion, fine-tuning pre-trained models allows us to leverage the knowledge and representations learned from extensive data while tailoring them to solve our specific machine learning tasks efficiently. It offers benefits such as time and resource efficiency, improved performance, and data efficiency
. By following a systematic approach and understanding the nuances of fine-tuning, we can unlock the full potential of pre-trained models and tackle a wide range of real-world problems. Now that you have a comprehensive understanding of what fine-tuning is, why it is used, and how it can be done, you can start exploring this technique in your own machine learning projects
. Remember to choose the right pre-trained model, make the necessary adjustments to the architecture, freeze or unfreeze layers strategically, train with a smaller learning rate, and experiment with different fine-tuning strategies. With practice and experience, you will be able to fine-tune models effectively and achieve impressive results in your machine learning endeavors.
Generative artificial intelligence, or GenAI, uses sophisticated algorithms to organize large, complex data sets into meaningful clusters of information in order to create new content, including text, images and audio, in response to a query or prompt. GenAI typically does two things: First, it encodes a collection of existing information into a form (vector space) that maps data points based on the strength of their correlations (dependencies)
. Second, when prompted, it then generates (decodes) new content by finding the correct context within the existing dependencies in the vector space. Familiar to users through popular interfaces such as OpenAI's ChatGPT and Google's Gemini, generative AI can answer complex questions, summarize vast amounts of information, and automate many tasks done previously by humans
. For example, businesses use generative AI to help draft reports, personalize marketing campaigns, make commercial films and improve code. Software vendors are integrating generative AI into core business applications, such as CRM and ERP, to boost efficiency and improve decision-making. GenAI is also being added to existing automation software, such as robotic process automation (RPA) and customer service chatbots, to make them more proactive
. Under the hood, generative AI is being used to create synthetic data to train other AI and machine learning models. GenAI takes off The strong interest in generative AI today from consumers, businesses and industry players alike was sparked by the blockbuster debut of ChatGPT in late 2022, which enabled users to create high-quality text in seconds and -- seemingly overnight -- became the fastest-growing consumer app in history
. The underpinning of this breakthrough technology, it should be noted, was not brand-new, dating back to the 1960s when it was introduced in chatbots. It wasn't until 2014, however, with the introduction of generative adversarial networks (GANs) -- a type of machine learning algorithm -- that generative AI could create convincingly authentic images, videos and audio of real people
. Two additional recent advances have played a critical role in generative AI going mainstream: transformers and the breakthrough language models they enabled. Transformers are a type of machine learning that made it possible for researchers to train ever-larger models without having to label all the data in advance. New, so-called large language models (LLMs) could thus be trained on billions of pages of text, resulting in answers with more depth
. In addition, transformers unlocked a new notion called attention that enabled models to track the connections between words across pages, chapters and books rather than just individual sentences. And they don't just analyze words; transformers can also use their ability to track connections to analyze code, security event data, proteins, chemicals and DNA. Rapid advances in large generative AI models -- i.e
.e., models with billions of parameters -- in turn, opened a new era in which generative AI models could not only write engaging text but also paint photorealistic images and even create somewhat entertaining sitcoms on the fly. Innovations in multimodal AI now enable users to generate content across multiple media types, including text, graphics and video
. This is the basis for visual tools like OpenAI's Dall-E and Google's Imagen 3, which convert text into images, and Janus Pro from Chinese AI startup DeepSeek, which can create images from a text description and generate text captions from images. These breakthroughs notwithstanding, we are still in relatively early -- and volatile -- days for generative AI. Implementations continue to have issues with accuracy and bias, as well as being prone to hallucinations and spitting back weird answers
. Generative AI has also unlocked concerns about deepfakes -- digitally forged images or videos -- and harmful cybersecurity attacks on businesses, including nefarious requests that realistically mimic an employee's boss
. From GenAI to AGI? GenAI supporters have claimed that generative techniques represent a significant step toward artificial general intelligence (AGI): AI that possesses all the intellectual capabilities humans possess, including reasoning, adaptability, self-improvement and understanding. Despite GenAI's impressive results, we are likely many technological advances short of that happening
. While GenAI excels at interpreting and generating content at one level of abstraction, it still struggles when parsing context across multiple levels of abstraction, resulting in various omissions and errors that are easily spotted by humans. This is why enterprises must proceed cautiously in how they implement these new techniques, whether via vendor tools, foundation models or on their own. Just as concerning: The pace of improvement has recently hit a wall. For example, OpenAI's GPT 4
. For example, OpenAI's GPT 4.5 LLM saw only modest improvements in accuracy despite a 10x to 30x increase in development costs. In addition, as models have become larger and more complex, vendors have struggled to scale them efficiently
. Making matters more complicated, new approaches exemplified in DeepSeek's R1 model suggest that far less computing is required to train new foundation models -- a finding that, following the model's January 2025 release, triggered a massive sell-off of shares in Nvidia and other AI-related tech stocks. Still, progress thus far has already resulted in generative AI fundamentally changing enterprise technology and transforming how businesses operate
. Let's look deeper at how this technology works and its implications. Hyperlinks throughout this overview of generative AI will take you to articles, tips and definitions providing even more detailed explanations. How does generative AI work? The generative AI process starts with foundation models, such as the GPT series, Palm and Gemini. These are large neural networks trained on massive collections of data that provide a broad assimilation of known information and knowledge
. They generally include text, which provides a way to distill human concepts. Other data sources include images, video, IoT, robot instructions and enterprise data. As noted, basic generative AI models consist of an encoder and a decoder. The encoder transforms text, code, images and other prompts into a format AI can process. This intermediate representation could be a vector embedding or a probabilistic latent space
. The decoder generates content by transforming the intermediate representation into new content, such as a chatbot response, document summary, translation or other data type. Algorithmic models to train GenAI A variety of algorithms are used to train the encoder and decoder components. For example, the transformer algorithms popular with developers of large language models use self-attention algorithms that learn and refine vector embeddings that capture the semantic similarity of words
. Self-attention algorithms aggregate the embeddings in an intermediate representation that encodes the context of the input. The decoder uses an autoregressive method, like time series analysis, to transform the intermediate representation into a response. Diffusion models, employed in image generation, use forward diffusion to transform data into a latent space representation and reverse diffusion to generate new content
. Variational autoencoders (VAEs) use other techniques to encode and decode data into a probabilistic latent space, where each point is defined by a range of possibilities. The aforementioned GANs use one neural network, such as a convolutional neural network, for generating realistic content and another discriminative model for detecting AI-generated content
. A Kolmogorov-Arnold Network (KAN), an emerging and very advanced neural network, creates a direct mapping from input to output without traditional encoders and decoders. Certain algorithmic architectures are better suited than others for specific types of prompts, which can come in the form of text, a code snippet, an enterprise data set, an image, a video, a design, musical notes or any input that the AI system can process
. Diffusion models, for example, excel at generating very high-quality, realistic images, including human faces. GANs are also good at creating realistic images but can be hard to train. Both GANs and VAEs are used to generate synthetic data for AI training. KANs, which excel at making complex functions understandable but are in the experimental stage, have the potential to be good at weather and stock market predictions
. Early versions of generative AI required submitting data via an API or an otherwise complicated process. Developers had to familiarize themselves with special tools and write applications using languages such as Python. Pioneers in generative AI have developed better user experiences that let you describe a request in plain language. After an initial response, you can also customize the results with feedback about the style, tone and other elements you want the generated content to reflect
. Image displaying quotes about generative AI from industry experts Best practices for using generative AI The best practices for using generative AI vary depending on the modalities, workflow and desired goals. That said, it is always important to consider factors such as accuracy, transparency and ease of use when working with generative AI. The following practices serve as a guide: Clearly label all generative AI content for users and consumers
. Assess the cost/benefit tradeoffs compared with other tools. Vet the accuracy of generated content using primary sources, where applicable. Consider how bias might get woven into generated AI results. Double-check the quality of AI-generated code and content using other tools. Learn the strengths and limitations of each generative AI tool. Familiarize yourself with common failure modes in results and work around them. Vet new applications with subject matter experts to identify problems
. Implement guardrails to mitigate issues with trust and security. ChatGPT, Gemini, Copilot and other GenAI tools Early GenAI tools focused on a single task, such as answering questions, summarizing documents, writing code or creating images. Major AI vendors like OpenAI, Google, and Microsoft now brand their GenAI products as general-purpose suites that support multiple tasks
. Other vendors continue to innovate with best-of-breed tools and APIs optimized for specific tasks or by offering better integration with other popular tools for software development, media production or enterprise apps. The following are some of the top GenAI tools: OpenAI ChatGPT. The AI-powered chatbot that took the world by storm in November 2022 was built on OpenAI's GPT-3.5 implementation
.5 implementation. OpenAI pioneered a way to fine-tune text responses via a chat interface equipped with interactive feedback. After the incredible popularity of the new GPT interface, Microsoft announced a significant new investment in OpenAI and integrated a version of GPT into its Bing search engine
. With its GPT-4o model, the company now supports multimodal capabilities for listening and responding with realistic voices, image prompting and advanced reasoning capabilities that improve accuracy and precision. Multimodal image generation capabilities include Dall-E for images and Sora for video. The ChatGPT search feature, launched in late 2024, lets users search the web within the ChatGPT interface. GPT-4
. GPT-4.5 has received a lukewarm reception because it cost 10 times more than previous models with only modest gains. Google Gemini. Google pioneered transformer AI techniques for processing language, proteins and other types of content. It now provides a suite of Gen AI tools via its Gemini interface to answer questions, summarize documents, search the web and analyze as well as generate code
. Google also streamlines access to AI models that generate other kinds of content, such as its Imagen diffusion-based model for images. Other GenAI capabilities are provided as part of its Vertex AI service for application development. NotebookLM enables users to upload documents, audio and video to summarize, answer questions and create short audio podcasts. Microsoft Copilot. Microsoft was an early investor in OpenAI and used the company's various LLMs to develop a range of GenAI tools
. It has since consolidated its GenAI branding into the Microsoft Copilot suite for Windows, Microsoft 365 and GitHub tools. The service now uses LLMs developed by Microsoft and third parties in addition to OpenAI's LLMs. Copilot excels at processing and generating content using apps such as Word, Excel, PowerPoint, GitHub and Microsoft Dynamics 365 CRM. Perplexity
. Perplexity. California startup Perplexity AI launched its AI-powered search engine by the same name in 2022 with the goal of improving the search and summarizing experience across webpages and science papers. Its latest Comet model uses an agentic AI approach to automate web tasks and streamline search. In addition to drawing upon its own Sonar LLM, Comet takes advantage of numerous GenAI models from other vendors, including OpenAI GPT-4o and Claude Sonnet
. Whereas other tools focus on improving foundation models, Perplexity has focused on improving the UX of working with existing models. Anthropic Claude. This company was founded by former OpenAI employees with the goal of developing more accurate, trustworthy and safe foundation models
. With its privacy-first focus, Claude uses a "Constitutional AI" approach that trains the foundation model to follow predefined ethical principles and weigh the results of different kinds of models in generating content as well as making or suggesting decisions. DeepSeek. A newer model developed by High-Flyer, a Chinese hedge fund, DeepSeek pushed the bar in terms of efficiency, performance and cost-effectiveness compared with traditional AI tools
. Experts estimated it was trained at one-tenth to one-thirtieth the cost of traditional foundation models while achieving comparable results. News of the new DeepSeek model caused a massive sell-off of U.S. tech stocks, particularly those in AI infrastructure and chips, followed by a partial rebound. Specialized GenAI tools A variety of best-of-breed commercial and open source tools also excel at generating particular kinds of content or for various use cases
. The following examples are a case in point: Text generation tools include Jasper, Writer and Lex. Image generation tools include Midjourney and Stable Diffusion. Music generation tools include Amper, Dadabots and MuseNet. Code generation tools include Amazon CodeWhisperer, Codia AI, CodeStarter, Codex and Tabnine. Voice synthesis tools include Descript, Listnr and PodcastAI. What are the business benefits of generative AI? Generative AI can be applied extensively across many areas of business
. It can make it easier to interpret and understand existing content as well as automatically create new content. Developers are exploring ways that generative AI can improve existing workflows, with an eye to adapting workflows entirely to take advantage of the technology. Some of the potential benefits of implementing generative AI include the following: Improving customer experience
. Chatbots can pull information from enterprise systems and technical documents to support a wide range of customer requests and can recommend specific offerings for upsell opportunities. They can also simplify many processes, such as ordering or changing products and services Building new products and accelerating development
. GenAI tools integrated into software development environments can analyze and refactor existing code bases, streamline code generation, streamline testing processes and support deployment as well as rollback processes. They can also make it easier for business and subject matter experts to implement new products, processes and features. Improving task efficiency
. Improving task efficiency. Office productivity tools and business applications, such as CRM and ERP applications, can use GenAI models to extract, copy and paste essential information across apps, services and databases to reduce key entry and improve accuracy. Boosting personalization. Content creation tools can customize offers, translate content for different languages or regions, suggest relevant upsell opportunities and distill the most relevant information for a given customer or inquiry
. Risk identification and management. GenAI capabilities can distill relevant information from various systems to identify, mitigate and resolve risks. Examples include improving IT service management, IT compliance, security audits and enterprise risk management. Implementing generative AI is not just about technology. Businesses must also consider its impact on people and processes
. What are the limitations of generative AI? Early implementations of generative AI vividly illustrate its many limitations. Some of the challenges generative AI presents result from the specific approaches used to implement particular use cases. For example, a generative AI-produced summary of a complex topic is easier to read than an explanation filled with various sources and citations that support key points
. The readability of the GenAI summary, however, comes at the expense of a user being able to vet where the information comes from. Consider the following limitations when implementing or using a generative AI app: The app does not always identify the source of content and sometimes points to plausible but erroneous sources of information. It can be challenging to assess the biases of original sources. Realistic-sounding content makes it harder to identify inaccurate information
. It can be difficult to understand how to tune for new circumstances. The app lacks the capability for original thought. What are the concerns surrounding generative AI? Many of the concerns raised by the current state of generative AI relate to the quality of results, the potential for misuse as well as abuse and the potential to disrupt existing business models. Major issues include the following: Misleading information
. The biggest problem with GenAI tools is that they can hallucinate and produce results not grounded in prompts. For example, Air Canada lost a lawsuit after its chatbot provided a passenger bereavement flight information about refunds that conflicted with the company's official policy. Fake citations. Numerous law firms have been sanctioned for filing briefs with nonexistent court precedents
. In one case, a prominent Stanford University communications professor and recognized expert on misinformation was called out for including fake citations generated by ChatGPT in an expert witness filing he provided -- ironically, for a case related to deepfake laws. The testimony was dismissed
. The testimony was dismissed. Academic papers have also been retracted for including fake citations: GenAI understands the format of a publication, but it can't tell the difference between a real publication and one that it has just made up. Copyright violations. Lawmakers and regulators are still deciding whether using copyrighted information is a violation of existing copyright laws meant to protect human research and the summarization of copyrighted content
. The numerous lawsuits that have been filed against GenAI developers have been largely unsuccessful. It's not clear how court losses or regulatory changes will affect enterprises that use these models. Terms of service violations. In the process of training, many AI models have scraped data and used APIs in ways that violated the terms of service (TOS) of publishers and other organizations. The issues are different from and somewhat overlap with the copyright violations cited above
. For example, OpenAI has been accused of violating the terms of service of publishers like the New York Times in aggregating content from The New York Times. Meanwhile. Microsoft and OpenAI have also claimed that the developers of the DeepSeek model violated their TOS during the training process. Like copyright, TOS violations could affect enterprise users of these systems, even if they have been open sourced
. Somewhat paradoxically, Microsoft now offers DeepSeek models on its Azure cloud platform. Openwashing. This refers to various deceptive practices used by GenAI companies to appear open and transparent about the AI models they are developing and marketing. Some AI models, for example, have been released under new licensing schemes that purport to be open but might force enterprises to pay licensing fees if they exceed a threshold of usage or compete with the developer
. Also, many open AI models lack transparency into the data used for training them, which is critical for understanding issues like bias. Data sweatshops. The rise of GenAI has driven the growth of a shadow economy of data sweatshops. Some of these employ low-wage workers in developing countries, with little regard for worker well-being. The most concerning examples include exposing contractors to toxic content that could cause PTSD. Energy concerns
. Energy concerns. The enthusiasm behind GenAI has accelerated the growth of large-scale data centers, which consume vast quantities of water and power. The infrastructure strains existing electric grid infrastructure and, in some cases, has raised utility rates for local residents. There is also concern that the advent of mega data centers slowed efforts to replace carbon energy plants with more sustainable factories. New security threats
. New security threats. Hackers are discovering a variety of creative ways to craft more effective attacks on enterprise systems. At a technical level, GenAI tools can identify potential vulnerabilities and craft better attacks on infrastructure and security systems. New social engineering attacks can craft more persuasive bogus emails or even mimic the voice and video of trusted executives to create high-value attacks on businesses and consumers
. What are the use cases for generative AI? Generative AI can be applied in an array of use cases across industries to generate content, summarize complex information and streamline various enterprise processes. The technology is becoming more accessible to users of all kinds, thanks to cutting-edge breakthroughs like GPT, diffusion models and GANs that can be tuned for different applications
. Some use cases for generative AI include the following: Implementing chatbots for customer service and technical support. Analyzing and summarizing events from security and IT service logs. Improving movie dubbing and educational content in different languages. Writing email responses, résumés and business reports. Prioritizing interview candidates from a collection of résumés. Creating photorealistic art for marketing and advertising. Improving product demonstration videos
. Improving product demonstration videos. Suggesting new drug compounds to test. Designing physical products and buildings. Optimizing new chip designs. Writing music in a specific style or tone. Creating podcasts for particular users, audiences or personas. Answering questions from product manuals. Augmenting and automating code generation and QA processes
. Use cases for generative AI, by industry New generative AI technologies have sometimes been described as general-purpose technologies akin to steam power, electricity and computing because they can profoundly affect many industries and support many use cases
. It's essential to remember that, like previous general-purpose technologies, it often took decades for organizations to find the best way to take advantage of the new technology and transform their workflows rather than, in the case of automation, simply repave the cow path. Below is a sampling of the ways in which generative AI applications are changing industries. Finance
. Finance. Generative AI could add $200 billion to $340 billion in annual value to banking, chiefly through productivity gain, according to consulting firm McKinsey. GenAI's ability to identify patterns in vast amounts of customer and market data is enabling banks to hyperpersonalize customer service and improve fraud detection. Legal services
. Legal services. Inundated with generative AI products, the legal industry is learning to effectively and safely use tools that are designed to do everything from legal research and summarizing briefs to preparing tax returns, drafting contracts and suggesting legal arguments. Manufacturing. GenAI models can integrate data from cameras, X-rays and other metrics to identify defective parts and the root causes, accelerating time to insight
. Plant operators can query in natural language to get comprehensive reports on internal and external operations. Education. Generative AI helps teachers and administrators through task automation, including grading assignments, generating quizzes and crafting individualized study programs
. GenAI's ability to find answers instantly is challenging educators to rethink teaching methods and focus on higher-order skills, such as critical thinking and problem-solving, as well as address the ethical issues posed by AI use. Ethics and bias in generative AI Despite their promise, the new generative AI tools open a can of worms regarding accuracy, trustworthiness, bias, hallucination and plagiarism -- ethical issues that likely will take years to sort out
. None of the issues are particularly new to AI. For example, Microsoft's first foray into chatbots in 2016, called Tay, had to be turned off after it started spewing inflammatory rhetoric on Twitter. Google was forced to turn off a controversial image-labeling service after it labeled African Americans as gorillas. What is new is that the latest crop of generative AI apps sounds more coherent on the surface
. But this combination of humanlike language and coherence is not synonymous with human intelligence, and there currently is great debate about whether generative AI models can be trained to have reasoning ability. One Google engineer was even fired after publicly declaring the company's generative AI app, Language Models for Dialog Applications (Lamda), was sentient. The convincing realism of generative AI content introduces a new set of AI risks
. It makes it harder to detect AI-generated content and, more importantly, makes it more difficult to detect errors. This can be a big problem when we rely on generative AI results to write code or provide medical advice. Many results of generative AI are not transparent, so it's hard to determine whether, for example, they infringe on copyrights or whether there's a problem with the original sources from which they draw results
. If you don't know how the AI arrived at a conclusion, you cannot determine why it might be wrong. In addition, data leakage of private and sensitive information and new compliance risks are two big concerns. A variety of trustworthy and ethical AI frameworks could help address such concerns. It remains unclear how generative AI business metrics for monitoring bias will change in the current political climate
. Generative AI history The Eliza chatbot created by Joseph Weizenbaum in the 1960s represented a major breakthrough in natural language processing (NLP) and was one of the earliest examples of generative AI. These early implementations used a rules-based approach that broke easily due to a limited vocabulary, lack of context and overreliance on patterns, among other shortcomings. Early chatbots were also difficult to customize and extend
. The field saw a resurgence in the wake of advances in neural networks and deep learning in 2010 that enabled the technology to automatically learn to parse existing text, classify image elements and transcribe audio. Ian Goodfellow introduced GANs in 2014. This deep learning technique provided a novel approach for organizing competing neural networks to generate and then rate content variations. These could generate realistic people, voices, music and text
. This inspired interest in -- and fear of -- how generative AI could be used to create realistic deepfakes that impersonate voices and people in videos. Since then, progress in other neural network techniques and architectures has helped expand generative AI capabilities. Techniques include VAEs, long short-term memory, transformers, diffusion models, Gaussian splatting and neural radiance fields
. Researchers and vendors are also starting to combine techniques in novel ways to improve accuracy, precision and performance while reducing hallucinations. Timeline of the history of generative AI technologies The future of generative AI The incredible depth and ease of ChatGPT spurred the widespread adoption of generative AI
. The rapid adoption of generative AI applications, to be sure, also demonstrated some of the difficulties in rolling out this technology safely, responsibly and to optimal effect. Indeed, despite the promise of GenAI techniques to radically change the way we work, enterprises are still in the early stages of assessing AI readiness and figuring out how GenAI's capabilities might improve results and reduce costs compared to traditional approaches
. Moreover, current approaches to GenAI are hitting a wall in terms of the cost required to improve performance and accuracy. In addition, GenAI struggles with questions that require synthesis not already embedded in its language model because it doesn't have the language to make that systematic step -- something humans do easily. But these early implementation issues and limitations have also inspired research into better tools. It's an incredibly dynamic and ambitious field
. It's an incredibly dynamic and ambitious field. AI vendors and developers are experimenting with new agentic AI frameworks, such as AutoGPT, that use prompt chaining to connect multiple special-purpose GenAI models to improve accuracy and reduce hallucinations. At the bleeding edge, new approaches to physical AI, exemplified by the Nvidia Omniverse and Cosmos platforms, apply GenAI techniques to analyze the physical world
. Moreover, the popularity of generative AI tools such as Midjourney, Stable Diffusion, Gemini and ChatGPT has fueled an endless variety of training courses at all levels of expertise, many of which are aimed at helping developers create better AI applications. Others focus more on helping business users apply the new technology more effectively across the enterprise
. At some point, industry and society will also build better tools for tracking the provenance of information to create more trustworthy AI. Meantime, generative AI will continue to evolve, making advancements in translation, drug discovery, anomaly detection and the generation of new content, from text and video to fashion design and music
. And as good as the new one-off tools are, the most significant impact of generative AI will come from integrating these capabilities directly into the tools we already use. Design tools will seamlessly embed more useful recommendations directly into our workflows. Training tools will be able to automatically identify best practices in one part of an organization to help train other employees more efficiently
. These are just a fraction of the ways generative AI will change what we do in the near term. What the impact of generative AI will be in the future is hard to say. But as we continue to harness these tools to automate and augment human tasks, we will inevitably find ourselves having to reevaluate the nature and value of human expertise. Generative AI FAQs Below are some frequently asked questions people have about generative AI
. Who created generative AI? Joseph Weizenbaum created the first generative AI in the 1960s as part of the Eliza chatbot. Ian Goodfellow demonstrated generative adversarial networks for generating realistic-looking and -sounding people in 2014. Subsequent research into LLMs from OpenAI and Google ignited the recent enthusiasm that has evolved into tools like ChatGPT, Google Gemini and Dall-E
. What's the difference between generative AI and traditional AI? Generative AI focuses on creating new and original content, chat responses, designs, synthetic data and even deepfakes. It's particularly valuable in creative fields and for novel problem-solving, as it can autonomously generate many types of new outputs. It relies on neural network techniques such as VAEs, GANs and transformers to predict text, pixels or video frames
. Generative AI often starts with a prompt that lets a user or data source submit a starting query or data set to guide content generation. This can be an iterative process to explore content variations. Traditional AI algorithms, on the other hand, often follow a predefined set of rules to process data and produce a result
. In the wake of GenAI, these older algorithms are sometimes called discriminative AI, since they discover patterns in data to yield recommendations or analytics insights and to make decisions. Both approaches have their strengths and weaknesses, depending on the problem to be solved, with generative AI being well-suited for tasks involving NLP and for the creation of new content, and traditional algorithms more effective for tasks involving rule-based processing and predetermined outcomes
. Traditional AI techniques tend to be faster, more efficient and less prone to hallucinations. Generative AI techniques are more flexible and tend to work better in discovering patterns across multiple modalities of data, such as text, audio and video. What's the difference between large language models and generative AI? Large language models are a type of generative AI designed for linguistic tasks, such as text generation, question and answering, and summarization
. The broad category of generative AI includes a variety of model architectures and data types, including video, images and audio. Learn more about their differences in this article comparing LLMs and generative AI. How do you build a generative AI model? A generative AI model starts by efficiently encoding a representation of what you want to generate
. For example, a generative AI model for text might begin by finding a way to represent the words as vectors that characterize the similarity between words often used in the same sentence or that mean similar things. Recent progress in LLM research has helped the industry implement the same process to represent patterns found in images, sounds, proteins, DNA, drugs and 3D designs
. This generative AI model provides an efficient way of representing the desired type of content and efficiently iterating on useful variations. How do you train a generative AI model? The generative AI model needs to be trained for a particular use case. The recent progress in LLMs provides an ideal starting point for customizing applications for different use cases
. For example, the popular GPT model developed by OpenAI has been used to write text, generate code and create imagery based on written descriptions. Training involves tuning the model's parameters for different use cases and then fine-tuning results on a given set of training data. For example, a call center might train a chatbot against the kinds of questions AI agents get from various customer types and the responses that service agents give in return
. An image-generating app, in distinction to text, might start with labels that describe content and style of images to train the model to generate new images. AI prompting and agent-based systems With the advent of autonomous AI agents such as AutoGPT and AgentGPT, the way machines operate and complete tasks is evolving and -- along with that -- the role of AI prompt engineers
. The following are some of the techniques that AI agents and prompt engineers use to enable more autonomous and informed generative AI. Chain-of-thought prompting aims to improve language model's performance by structuring the prompt to mimic how a human might reason through a problem. Queries using this technique use phrases like "explain your answer step-by-step" or "describe your reasoning in steps," with the aim of generating more accurate answers and reducing hallucinations
. Retrieval-augmented generation (RAG) and Retrieval-Augmented Language Model pretraining (RALM) are NLP techniques that improve the quality of large language models by retrieving data from external sources of knowledge, such as document repositories, vector databases and APIs. RAG retrieves the information in real time to answer a prompt, while RALM pretrains the LLM with retrieval capabilities to improve its knowledge during training
. LangChain is an open source framework that facilitates RAG by connecting LLMs with external knowledge sources and that provides the infrastructure for building LLM agents that can execute many of the tasks in RAG and RALM. What are some generative models for natural language processing? Some generative models for natural language processing include the following: Carnegie Mellon University's XLNet addressed limitations in BERT by improving on its pretraining method
. Google's ALBERT ("A Lite" BERT) focused on the reduction of parameters. Google BERT revolutionized NLP with transformer architecture. Google Lamda's training on dialogue led to more natural conversation. OpenAI's GPT is foundational to today's generative NLP. Word2Vec and GloVe word embedding models improved tasks such as sentiment analysis and translation
. What is conversational AI, and how is it different from predictive and generative AI? Conversational AI, a subset of GenAI, helps AI systems like virtual assistants, chatbots and customer service apps interact and engage with humans using natural dialogue. It uses techniques from NLP and machine learning to understand language and provide humanlike text or speech responses
. Predictive AI, in distinction to generative AI, uses patterns in historical data to forecast outcomes, classify events and provide actionable insights. Organizations use predictive AI to sharpen decision-making and develop data-driven strategies. How could generative AI replace jobs? Generative AI has the potential to replace a variety of jobs
. The following are a sample of the types of jobs vulnerable to GenAI: Content writers, particularly those who write formulaic content such as product descriptions, basic marketing content, summarizations and recaps. Graphic design and visual content creators. Customer service and support. Data processing work, including data entry, analysis and scheduling. Software development jobs, including code generation and software testing
. Some companies will look for opportunities to replace humans where possible, while others will use generative AI to augment and enhance their existing workforce. How is generative AI changing creative work? Generative AI promises to help creative workers explore variations of ideas. Artists might start with a basic design concept and then explore variations. Industrial designers could explore product variations
. Architects could explore different building layouts and visualize them as a starting point for further refinement. It could also help democratize some aspects of creative work. For example, business users could explore product marketing imagery using text descriptions. They could further refine these results using simple commands or suggestions
. Podcast generation tools like Google's NotebookLM can transform existing websites, PDFs, interviews and videos into interactive podcasts for employees and customers. What's next for generative AI? ChatGPT's ability to generate humanlike text has sparked widespread curiosity about generative AI's potential. It also shined a light on the many problems and challenges ahead. In the short term, work in generative AI will focus on improving the user experience and workflows
. Building trust in generative AI results will also be essential. More companies will customize generative AI on their own data to help improve branding and communication as well as enforce company-specific best practices for writing and formatting more readable and consistent code. Vendors will integrate generative AI capabilities into more of their tools to streamline content generation workflows in ways that increase productivity
. Generative AI will also play a role in various aspects of data processing, transformation, labeling and vetting as part of augmented analytics workflows. Semantic web applications will use generative AI to automatically map internal taxonomies describing job skills to different taxonomies on skills training and recruitment sites. Similarly, business teams will use these models to transform and label third-party data for more sophisticated risk assessments and opportunity analysis capabilities
. Generative AI models are being extended to support 3D modeling, product design, drug development, digital twins, supply chains and business processes. This is making it easier to generate new product ideas, experiment with different organizational models and explore various business ideas. Will AI ever gain consciousness? Some AI proponents believe that generative AI is an essential step toward general-purpose AI and even consciousness
. One early tester of Google's Lamda chatbot even created a stir when he publicly declared it was sentient. Then he was let go from the company. In 1993, the American science fiction writer and computer scientist Vernor Vinge posited that, in 30 years, we would have the technological ability to create a "superhuman intelligence" -- an AI that is more intelligent than humans -- after which the human era would end. AI pioneer Ray Kurzweil predicted such a "singularity" by 2045
. Many other AI experts think it could be much further off. Robot pioneer Rodney Brooks predicted that AI will not gain the sentience of a 6-year-old in his lifetime but could seem as intelligent and attentive as a dog by 2048.
